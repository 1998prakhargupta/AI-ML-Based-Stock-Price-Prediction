{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üõ† 2. Import Required Modules\n",
    "# =====================================\n",
    "import plotly.graph_objects as go\n",
    "import ta\n",
    "from breeze_connect import BreezeConnect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from app_config import Config\n",
    "from breeze_utils import BreezeDataManager\n",
    "from model_utils import create_time_features, calculate_technical_indicators\n",
    "\n",
    "# =====================================\n",
    "# üõ† 3. Initialize Secure Data Manager\n",
    "# =====================================\n",
    "# Initialize secure configuration and data manager\n",
    "config = Config()\n",
    "breeze_manager = BreezeDataManager()\n",
    "\n",
    "# Set up Google Drive if in Colab environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Not in Colab environment, skipping Google Drive mount\")\n",
    "\n",
    "# Authenticate with Breeze API\n",
    "if breeze_manager.authenticate():\n",
    "    print(\"‚úÖ Breeze API authenticated successfully\")\n",
    "    breeze = breeze_manager.breeze\n",
    "else:\n",
    "    print(\"‚ùå Failed to authenticate with Breeze API\")\n",
    "    raise Exception(\"Authentication failed\")\n",
    "\n",
    "# =====================================\n",
    "# üõ† 4. Helper Functions (Now Using Utilities)\n",
    "# =====================================\n",
    "\n",
    "# üìÖ Date formatters\n",
    "def get_date_iso(days_ago=0):\n",
    "    date = datetime.now() - timedelta(days=days_ago)\n",
    "    date = date.replace(hour=9, minute=0, second=0, microsecond=0)\n",
    "    return date.isoformat() + \".000Z\"\n",
    "\n",
    "def get_end_date_iso():\n",
    "    date = datetime.now().replace(hour=15, minute=30, second=0, microsecond=0)\n",
    "    return date.isoformat() + \".000Z\"\n",
    "\n",
    "# def get_last_trading_day(days_back=0):\n",
    "#     \"\"\"Returns the last trading day (skips weekends and holidays)\"\"\"\n",
    "#     date = datetime.now() - timedelta(days=days_back)\n",
    "#     attempts = 0\n",
    "#     max_attempts = 10  # To prevent infinite loops\n",
    "\n",
    "#     while attempts < max_attempts:\n",
    "#         # Skip weekends\n",
    "#         if date.weekday() >= 5:  # 5=Saturday, 6=Sunday\n",
    "#             date -= timedelta(days=1)\n",
    "#             attempts += 1\n",
    "#             continue\n",
    "\n",
    "#         # Verify trading day by checking if we can get index data\n",
    "#         try:\n",
    "#             response = breeze.get_historical_data_v2(\n",
    "#                 interval=\"1day\",\n",
    "#                 from_date=date.replace(hour=9, minute=0, second=0, microsecond=0).isoformat() + \".000Z\",\n",
    "#                 to_date=date.replace(hour=15, minute=30, second=0, microsecond=0).isoformat() + \".000Z\",\n",
    "#                 stock_code=\"NIFTY\",\n",
    "#                 exchange_code=\"NSE\",\n",
    "#                 product_type=\"cash\"\n",
    "#             )\n",
    "#             if response['Status'] == 200:\n",
    "#                 return date\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         date -= timedelta(days=1)\n",
    "#         attempts += 1\n",
    "\n",
    "#     return datetime.now()  # Fallback to today if can't determine\n",
    "\n",
    "def detect_strike_step(strikes):\n",
    "    if len(strikes) < 2:\n",
    "        return None\n",
    "    return int(min(np.diff(sorted(strikes))))\n",
    "\n",
    "def get_nearest_strike_price(ltp, valid_step):\n",
    "    if ltp:\n",
    "      return int(round(ltp / valid_step) * valid_step)\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "# üìà Fetch LTP\n",
    "def get_live_ltp(stock_code, exchange_code):\n",
    "    response = breeze.get_quotes(stock_code=stock_code, exchange_code=exchange_code)\n",
    "    if response['Status'] == 200:\n",
    "        ltp = float(response['Success'][0]['ltp'])\n",
    "        print(f\"üì¶ Last Traded Price for {stock_code} is {ltp}\")\n",
    "        return ltp\n",
    "    else:\n",
    "        print(f\"‚ùå Error fetching LTP: {response['Error']}\")\n",
    "        return None\n",
    "\n",
    "# üìà Fetch Historical Data\n",
    "def fetch_historical_data(stock_code, exchange_code, product_type, interval, from_date, to_date, file_name, expiry_date=None, strike_price=None, right=None):\n",
    "    print(f\"Fetching {product_type.upper()} data for {stock_code} with Strike Price = {strike_price}\")\n",
    "\n",
    "    params = {\n",
    "        \"interval\": interval,\n",
    "        \"from_date\": from_date,\n",
    "        \"to_date\": to_date,\n",
    "        \"stock_code\": stock_code,\n",
    "        \"exchange_code\": exchange_code,\n",
    "        \"product_type\": product_type\n",
    "    }\n",
    "\n",
    "    if product_type == \"futures\" and expiry_date:\n",
    "        params[\"expiry_date\"] = expiry_date\n",
    "        print(f\"Fecting data for FUTURE with Expiry: {expiry_date}\")\n",
    "\n",
    "    if product_type == \"options\":\n",
    "        if not all([expiry_date, strike_price, right]):\n",
    "            print(\"‚ùå Missing expiry/strike/right for options.\")\n",
    "            return\n",
    "        params[\"expiry_date\"] = expiry_date\n",
    "        params[\"strike_price\"] = strike_price\n",
    "        params[\"right\"] = right.lower()\n",
    "        print(f\"Fecting data for OPTIONS for Strike Price:{strike_price} with Expiry: {expiry_date}\")\n",
    "\n",
    "    response = breeze.get_historical_data_v2(**params)\n",
    "\n",
    "    if response['Status'] == 200:\n",
    "        data = pd.DataFrame(response['Success'])\n",
    "        data.to_csv(os.path.join(save_path, file_name), index=False)\n",
    "        print(f\"‚úÖ Data saved to {file_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error fetching data: {response['Error']}\")\n",
    "\n",
    "# üìà Simple Trend Detection (Bullish/Bearish)\n",
    "def detect_trend(stock_code, exchange_code, lookback_days=7):\n",
    "    from_date = get_last_trading_day(days_back=lookback_days)\n",
    "    to_date = get_last_trading_day(days_back=0)\n",
    "\n",
    "    from_date_iso = from_date.replace(hour=9, minute=0, second=0, microsecond=0).isoformat() + \".000Z\"\n",
    "    to_date_iso = to_date.replace(hour=15, minute=30, second=0, microsecond=0).isoformat() + \".000Z\"\n",
    "\n",
    "    candles = breeze.get_historical_data_v2(\n",
    "        interval=\"1day\",\n",
    "        from_date=from_date_iso,\n",
    "        to_date=to_date_iso,\n",
    "        stock_code=stock_code,\n",
    "        exchange_code=exchange_code,\n",
    "        product_type=\"cash\"\n",
    "    )\n",
    "\n",
    "    if candles['Status'] != 200:\n",
    "        print(f\"‚ùå Error fetching trend data: {candles['Error']}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(candles['Success'])\n",
    "    df['open'] = df['open'].astype(float)\n",
    "    df['close'] = df['close'].astype(float)\n",
    "\n",
    "    trend_open = df.iloc[0]['open']\n",
    "    trend_close = df.iloc[-1]['close']\n",
    "\n",
    "    if trend_close > trend_open:\n",
    "        print(f\"üìà Bullish Trend: Open={trend_open} ‚û°Ô∏è Close={trend_close}\")\n",
    "        return \"call\"\n",
    "    else:\n",
    "        print(f\"üìâ Bearish Trend: Open={trend_open} ‚û°Ô∏è Close={trend_close}\")\n",
    "        return \"put\"\n",
    "\n",
    "def get_actual_expiry_dates(stock_code, max_weeks=4):\n",
    "    \"\"\"Get valid expiry dates by checking historical data availability\"\"\"\n",
    "    today = datetime.today()\n",
    "    valid_expiries = []\n",
    "\n",
    "    for i in range(max_weeks):\n",
    "        days_ahead = ((3 - today.weekday()) % 7) + (i * 7)  # Next Thursdays\n",
    "        potential_expiry = today + timedelta(days=days_ahead)\n",
    "        expiry_str = potential_expiry.strftime('%Y-%m-%d')\n",
    "\n",
    "        print(f\"üîç Checking expiry: {expiry_str}\")\n",
    "\n",
    "        try:\n",
    "            # Check using historical data API which is available in 1.0.62\n",
    "            response = breeze.get_historical_data_v2(\n",
    "                interval=\"30minute\",\n",
    "                from_date=get_last_trading_day(5).strftime('%Y-%m-%d') + \"T09:00:00.000Z\",\n",
    "                to_date=get_last_trading_day(0).strftime('%Y-%m-%d') + \"T15:45:00.000Z\",\n",
    "                stock_code=stock_code,\n",
    "                exchange_code=\"NFO\",\n",
    "                product_type=\"futures\",\n",
    "                expiry_date=expiry_str\n",
    "            )\n",
    "\n",
    "            if response['Status'] == 200 and response['Success']:\n",
    "                valid_expiries.append(expiry_str)\n",
    "                print(f\"‚úÖ Valid expiry found: {expiry_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error checking expiry {expiry_str}: {str(e)}\")\n",
    "\n",
    "    return valid_expiries\n",
    "\n",
    "def get_next_valid_expiry(stock_code, product_type=\"futures\", max_weeks=4):\n",
    "    \"\"\"Find next valid expiry using available methods\"\"\"\n",
    "    valid_expiries = get_actual_expiry_dates(stock_code, max_weeks)\n",
    "\n",
    "    if not valid_expiries:\n",
    "        print(f\"‚ùå No valid expiry found in next {max_weeks} weeks\")\n",
    "        return None\n",
    "\n",
    "    # Return nearest expiry\n",
    "    nearest_expiry = valid_expiries[0]\n",
    "    print(f\"üìå Using expiry: {nearest_expiry}\")\n",
    "    return nearest_expiry\n",
    "\n",
    "def get_valid_option_strikes(stock_code, expiry_date):\n",
    "    \"\"\"Get all valid strike prices for a given expiry\"\"\"\n",
    "    if not expiry_date:\n",
    "        print(\"‚ùå No expiry date provided\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # First get LTP to estimate reasonable strikes\n",
    "        ltp = get_live_ltp(stock_code, \"NSE\")\n",
    "        if not ltp:\n",
    "            print(\"‚ùå Couldn't get LTP for strike estimation\")\n",
    "            return []\n",
    "\n",
    "        # Get strikes for both calls and puts near the money\n",
    "        all_strikes = set()\n",
    "\n",
    "        for right in ['call', 'put']:\n",
    "            # Get chain for this right with estimated ATM strike\n",
    "            atm_strike = get_nearest_strike_price(ltp, 20)  # Assuming 50 strike interval\n",
    "\n",
    "            chain_response = breeze.get_option_chain_quotes(\n",
    "                stock_code=stock_code,\n",
    "                exchange_code=\"NFO\",\n",
    "                product_type=\"options\",\n",
    "                expiry_date=expiry_date + \"T06:00:00.000Z\",\n",
    "                right=right\n",
    "            )\n",
    "\n",
    "            if chain_response['Status'] == 200:\n",
    "                for item in chain_response.get('Success', []):\n",
    "                    try:\n",
    "                        all_strikes.add(float(item['strike_price']))\n",
    "                    except (KeyError, ValueError):\n",
    "                        continue\n",
    "\n",
    "        if not all_strikes:\n",
    "            print(\"‚ö†Ô∏è No strikes found in option chain\")\n",
    "            return []\n",
    "\n",
    "        print(f\"‚úÖ Found {len(all_strikes)} strikes for {expiry_date}\")\n",
    "        return sorted(all_strikes)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in get_valid_option_strikes: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def fetch_all_options_data(stock_code, exchange_code, expiry_date, ltp, interval, from_date, to_date, strike_range=800):\n",
    "    \"\"\"Fetch all options within range using SDK 1.0.62 compatible methods\"\"\"\n",
    "    if not expiry_date:\n",
    "        print(\"‚ùå No expiry date provided\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nüì¶ Fetching ALL options data for {stock_code} expiry {expiry_date}\")\n",
    "    print(f\"   Strike Range: {ltp-strike_range} to {ltp+strike_range}\")\n",
    "\n",
    "    all_options_data = []\n",
    "\n",
    "    # Determine strike step (TCS usually 20, NIFTY 50)\n",
    "    strike_step = 20 if stock_code == \"TCS\" else 50\n",
    "\n",
    "    # Calculate nearest strike\n",
    "    atm_strike = round(ltp / strike_step) * strike_step\n",
    "\n",
    "    # Generate strikes in range\n",
    "    min_strike = atm_strike - strike_range\n",
    "    max_strike = atm_strike + strike_range\n",
    "    strikes = range(\n",
    "        int(min_strike // strike_step * strike_step),\n",
    "        int(max_strike // strike_step * strike_step) + strike_step,\n",
    "        strike_step\n",
    "    )\n",
    "\n",
    "    print(f\"üî¢ Fetching {len(strikes)} strikes from {min(strikes)} to {max(strikes)}\")\n",
    "\n",
    "    for strike in strikes:\n",
    "        for right in ['call', 'put']:\n",
    "            print(f\"üîÑ Fetching {right.upper()} {strike}...\", end=' ')\n",
    "            try:\n",
    "                response = breeze.get_historical_data_v2(\n",
    "                    interval=interval,\n",
    "                    from_date=from_date,\n",
    "                    to_date=to_date,\n",
    "                    stock_code=stock_code,\n",
    "                    exchange_code=exchange_code,\n",
    "                    product_type=\"options\",\n",
    "                    expiry_date=expiry_date,\n",
    "                    strike_price=strike,\n",
    "                    right=right\n",
    "                )\n",
    "\n",
    "                if response['Status'] == 200:\n",
    "                    df = pd.DataFrame(response['Success'])\n",
    "                    if not df.empty:\n",
    "                        df['strike'] = strike\n",
    "                        df['right'] = right\n",
    "                        df['expiry_date'] = expiry_date\n",
    "                        all_options_data.append(df)\n",
    "                        print(f\"‚úÖ {right.upper()} {strike} fetched ({len(df)} records)\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è No data\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch {right.upper()} {strike}: {response['Error']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error fetching {right.upper()} {strike}: {str(e)}\")\n",
    "\n",
    "    if all_options_data:\n",
    "        combined_df = pd.concat(all_options_data, ignore_index=True)\n",
    "\n",
    "        # Add derived columns\n",
    "        combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n",
    "        combined_df['date'] = combined_df['datetime'].dt.date\n",
    "        combined_df['time'] = combined_df['datetime'].dt.time\n",
    "\n",
    "        expiry_date_str = pd.to_datetime(expiry_date).strftime('%d%b%y').upper()\n",
    "        combined_df['symbol'] = (\n",
    "            stock_code +\n",
    "            expiry_date_str +\n",
    "            combined_df['strike'].astype(int).astype(str) +\n",
    "            combined_df['right'].str[0].str.upper()\n",
    "        )\n",
    "\n",
    "        print(\"\\nüî¢ Normalizing records across all options...\")\n",
    "\n",
    "        # 1. Find the most complete option (with most timestamps)\n",
    "        option_counts = combined_df.groupby(['strike', 'right']).size()\n",
    "        max_records = option_counts.max()\n",
    "        print(f\"Maximum records for any option: {max_records}\")\n",
    "\n",
    "        # 2. Get the complete set of timestamps from the most complete option\n",
    "        complete_option = option_counts.idxmax()\n",
    "        complete_timestamps = combined_df[\n",
    "            (combined_df['strike'] == complete_option[0]) &\n",
    "            (combined_df['right'] == complete_option[1])\n",
    "        ]['datetime'].sort_values().unique()\n",
    "\n",
    "        # 3. Normalize each option to these timestamps\n",
    "        normalized_dfs = []\n",
    "\n",
    "        for (strike, right), group in combined_df.groupby(['strike', 'right']):\n",
    "            # Create complete index for this option\n",
    "            option_df = pd.DataFrame({'datetime': complete_timestamps})\n",
    "\n",
    "            # Merge with actual data\n",
    "            option_df = option_df.merge(\n",
    "                group,\n",
    "                on='datetime',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # Fill in static columns\n",
    "            option_df['strike'] = strike\n",
    "            option_df['right'] = right\n",
    "            option_df['expiry_date'] = expiry_date\n",
    "            option_df['symbol'] = f\"{stock_code}{expiry_date_str}{strike}{right[0].upper()}\"\n",
    "\n",
    "            # Re-add date/time columns\n",
    "            option_df['date'] = option_df['datetime'].dt.date\n",
    "            option_df['time'] = option_df['datetime'].dt.time\n",
    "\n",
    "            normalized_dfs.append(option_df)\n",
    "\n",
    "        # Combine all normalized data\n",
    "        normalized_df = pd.concat(normalized_dfs, ignore_index=True)\n",
    "\n",
    "        # Calculate statistics\n",
    "        num_options = len(normalized_df['symbol'].unique())\n",
    "        records_per_option = len(normalized_df) / num_options\n",
    "\n",
    "        print(f\"‚úÖ Normalized to {len(normalized_df)} total records\")\n",
    "        print(f\"   {records_per_option:.0f} records per option ({num_options} options)\")\n",
    "\n",
    "        # Save to file\n",
    "        file_path = os.path.join(save_path, \"tcs_options_data.csv\")\n",
    "        normalized_df.to_csv(file_path, index=False)\n",
    "        print(f\"\\n‚úÖ‚úÖ ALL options data saved to {file_path}\")\n",
    "        print(f\"   Total Records: {len(normalized_df)}\")\n",
    "        print(f\"   From: {normalized_df['datetime'].min()} to {normalized_df['datetime'].max()}\")\n",
    "        return normalized_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No options data was fetched\")\n",
    "        return None\n",
    "\n",
    "# =====================================\n",
    "# üõ† 5. Parameter Setup\n",
    "# =====================================\n",
    "\n",
    "# Use methods from BreezeDataManager for date operations\n",
    "get_date_iso = breeze_manager.get_date_iso\n",
    "get_end_date_iso = breeze_manager.get_end_date_iso\n",
    "get_last_trading_day = lambda days_back=0: breeze_manager.get_last_trading_day(days_back)\n",
    "\n",
    "print(\"‚úÖ All modules and utilities loaded successfully!\")\n",
    "\n",
    "stock_name = \"TCS\"\n",
    "interval = \"5minute\"\n",
    "\n",
    "# Use trading-day aware dates\n",
    "last_trading_day = get_last_trading_day(0)\n",
    "\n",
    "from_date = get_last_trading_day(30).strftime('%Y-%m-%d') + \"T09:00:00.000Z\"\n",
    "to_date = last_trading_day.strftime('%Y-%m-%d') + \"T15:30:00.000Z\"\n",
    "\n",
    "# Get LTP\n",
    "# ltp = get_live_ltp(stock_code=stock_name, exchange_code=\"NSE\")\n",
    "\n",
    "# ‚úÖ 2. Assume default strike step (TCS usually has 20)\n",
    "default_strike_step = 20 if stock_name.upper() == \"TCS\" else 100 # Or make a mapping\n",
    "\n",
    "# Detect based on past week\n",
    "option_type = detect_trend(stock_code=stock_name, exchange_code=\"NSE\", lookback_days=7) # \"call\" or \"put\"\n",
    "# Or for past month\n",
    "# option_type = detect_trend(stock_code=stock_name, exchange_code=\"NSE\", lookback_days=30)\n",
    "\n",
    "expiry_date_str = get_next_valid_expiry(stock_name)\n",
    "if not expiry_date_str:\n",
    "    print(\"‚ùå Could not determine valid expiry date\")\n",
    "    exit()\n",
    "\n",
    "ltp = get_live_ltp(stock_code=stock_name, exchange_code=\"NSE\")\n",
    "if not ltp:\n",
    "    print(\"‚ùå Could not get LTP\")\n",
    "    exit()\n",
    "\n",
    "# Now get valid strikes for options\n",
    "valid_strikes = get_valid_option_strikes(stock_name, expiry_date_str)\n",
    "\n",
    "if not valid_strikes:\n",
    "    print(\"‚ö†Ô∏è Could not get option strikes. Using default strike step.\")\n",
    "    strike_step = default_strike_step\n",
    "    strike_price = get_nearest_strike_price(ltp, strike_step)\n",
    "    valid_strikes = [strike_price]  # Fallback to just this strike\n",
    "else:\n",
    "    strike_step = detect_strike_step(valid_strikes) or default_strike_step\n",
    "    strike_price = get_nearest_strike_price(ltp, strike_step)\n",
    "\n",
    "print(f\"üìå Final parameters: {expiry_date_str=}, {strike_price=}, {valid_strikes=}\")\n",
    "\n",
    "# =====================================\n",
    "# üõ† 6. Fetch and Save Data\n",
    "# =====================================\n",
    "\n",
    "# 1Ô∏è‚É£ Fetch Equity\n",
    "fetch_historical_data(\n",
    "    stock_code=stock_name,\n",
    "    exchange_code=\"NSE\",\n",
    "    product_type=\"cash\",\n",
    "    interval=interval,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    file_name=\"tcs_equity_data.csv\"\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Fetch Futures\n",
    "futures_expiry_str = get_next_valid_expiry(\n",
    "    stock_code=stock_name,\n",
    "    product_type=\"futures\"\n",
    ")\n",
    "\n",
    "if futures_expiry_str:\n",
    "    fetch_historical_data(\n",
    "        stock_code=stock_name,\n",
    "        exchange_code=\"NFO\",\n",
    "        product_type=\"futures\",\n",
    "        interval=interval,\n",
    "        from_date=from_date,\n",
    "        to_date=to_date,\n",
    "        file_name=\"tcs_futures_data.csv\",\n",
    "        expiry_date=futures_expiry_str\n",
    "    )\n",
    "\n",
    "# 3Ô∏è‚É£ Fetch Options (Auto Strike)\n",
    "# Fetch full option chain instead of one strike\n",
    "options_df = fetch_all_options_data(\n",
    "    stock_code=stock_name,\n",
    "    exchange_code=\"NFO\",\n",
    "    expiry_date=expiry_date_str,\n",
    "    ltp=ltp,\n",
    "    interval=interval,\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    strike_range=800\n",
    ")\n",
    "\n",
    "if options_df is None:\n",
    "    print(\"‚ö†Ô∏è Warning: No options data was fetched\")\n",
    "\n",
    "# üéØ Done\n",
    "print(\"‚úÖ‚úÖ‚úÖ All data fetched and saved to Google Drive successfully!\")\n",
    "\n",
    "# =====================================\n",
    "# üõ† 7. Technical Indicator Generator\n",
    "# =====================================\n",
    "def add_all_technical_indicators(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure numeric columns\n",
    "    numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').ffill()\n",
    "    \n",
    "    # ===== TREND INDICATORS =====\n",
    "    # ADX Family\n",
    "    try:\n",
    "        df['ADX_14'] = ta.trend.adx(df['high'], df['low'], df['close'])\n",
    "        df['ADX_pos'] = ta.trend.adx_pos(df['high'], df['low'], df['close'])\n",
    "        df['ADX_neg'] = ta.trend.adx_neg(df['high'], df['low'], df['close'])\n",
    "    except Exception as e:\n",
    "        print(f\"ADX Error: {str(e)}\")\n",
    "\n",
    "    # Aroon\n",
    "    try:\n",
    "        df['Aroon_Up'] = ta.trend.aroon_up(df['high'], df['low'])\n",
    "        df['Aroon_Down'] = ta.trend.aroon_down(df['high'], df['low'])\n",
    "        df['Aroon_Osc'] = df['Aroon_Up'] - df['Aroon_Down']\n",
    "    except Exception as e:\n",
    "        print(f\"Aroon Error: {str(e)}\")\n",
    "\n",
    "    # MACD\n",
    "    try:\n",
    "        macd = ta.trend.MACD(df['close'])\n",
    "        df['MACD'] = macd.macd()\n",
    "        df['MACD_signal'] = macd.macd_signal()\n",
    "        df['MACD_diff'] = macd.macd_diff()\n",
    "        df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    except Exception as e:\n",
    "        print(f\"MACD Error: {str(e)}\")\n",
    "\n",
    "    # Ichimoku Cloud\n",
    "    try:\n",
    "        ichimoku = ta.trend.IchimokuIndicator(df['high'], df['low'])\n",
    "        df['Ichimoku_conv'] = ichimoku.ichimoku_conversion_line()\n",
    "        df['Ichimoku_base'] = ichimoku.ichimoku_base_line()\n",
    "        df['Ichimoku_a'] = ichimoku.ichimoku_a()\n",
    "        df['Ichimoku_b'] = ichimoku.ichimoku_b()\n",
    "    except Exception as e:\n",
    "        print(f\"Ichimoku Error: {str(e)}\")\n",
    "\n",
    "    # Moving Averages\n",
    "    ma_windows = [5, 10, 20, 50, 100, 200]\n",
    "    for window in ma_windows:\n",
    "        try:\n",
    "            df[f'SMA_{window}'] = ta.trend.sma_indicator(df['close'], window=window)\n",
    "        except Exception as e:\n",
    "            print(f\"SMA_{window} Error: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            df[f'EMA_{window}'] = ta.trend.ema_indicator(df['close'], window=window)\n",
    "        except Exception as e:\n",
    "            print(f\"EMA_{window} Error: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            df[f'WMA_{window}'] = ta.trend.wma_indicator(df['close'], window=window)\n",
    "        except Exception as e:\n",
    "            print(f\"WMA_{window} Error: {str(e)}\")\n",
    "\n",
    "    # ===== MOMENTUM INDICATORS =====\n",
    "    # RSI Family\n",
    "    rsi_windows = [7, 14, 21, 28]\n",
    "    for window in rsi_windows:\n",
    "        try:\n",
    "            df[f'RSI_{window}'] = ta.momentum.rsi(df['close'], window=window)\n",
    "        except Exception as e:\n",
    "            print(f\"RSI_{window} Error: {str(e)}\")\n",
    "\n",
    "    # Stochastic\n",
    "    try:\n",
    "        stoch = ta.momentum.StochasticOscillator(df['high'], df['low'], df['close'])\n",
    "        df['Stoch_%K'] = stoch.stoch()\n",
    "        df['Stoch_%D'] = stoch.stoch_signal()\n",
    "        df['Stoch_RSI'] = ta.momentum.stochrsi(df['close'])\n",
    "    except Exception as e:\n",
    "        print(f\"Stochastic Error: {str(e)}\")\n",
    "\n",
    "    # Other Momentum\n",
    "    momentum_indicators = [\n",
    "        ('CCI', lambda: ta.trend.cci(df['high'], df['low'], df['close'])),\n",
    "        ('DPO', lambda: ta.trend.dpo(df['close'])),\n",
    "        ('KST', lambda: ta.trend.kst(df['close'])),\n",
    "        ('KST_sig', lambda: ta.trend.kst_sig(df['close'])),\n",
    "\n",
    "        ('PSAR_up', lambda: ta.trend.psar_up(df['high'], df['low'], df['close'])),\n",
    "        ('PSAR_down', lambda: ta.trend.psar_down(df['high'], df['low'], df['close'])),\n",
    "        ('PSAR_up_indicator', lambda: ta.trend.psar_up_indicator(df['high'], df['low'], df['close'])),\n",
    "        ('PSAR_down_indicator', lambda: ta.trend.psar_down_indicator(df['high'], df['low'], df['close'])),\n",
    "        ('TRIX', lambda: ta.trend.trix(df['close'])),\n",
    "\n",
    "        ('TSI', lambda: ta.momentum.tsi(df['close'])),\n",
    "        ('ROC', lambda: ta.momentum.roc(df['close'])),\n",
    "        ('PPO', lambda: ta.momentum.ppo(df['close'])),\n",
    "        ('PVO', lambda: ta.momentum.pvo(df['volume'])),\n",
    "        ('KAMA', lambda: ta.momentum.kama(df['close'])),\n",
    "        ('WILLR', lambda: ta.momentum.williams_r(df['high'], df['low'], df['close']))\n",
    "    ]\n",
    "    \n",
    "    for name, func in momentum_indicators:\n",
    "        try:\n",
    "            df[name] = func()\n",
    "        except Exception as e:\n",
    "            print(f\"{name} Error: {str(e)}\")\n",
    "\n",
    "    # ===== VOLATILITY INDICATORS =====\n",
    "    try:\n",
    "        df['ATR_14'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'])\n",
    "    except Exception as e:\n",
    "        print(f\"ATR/NATR Error: {str(e)}\")\n",
    "\n",
    "    # Bollinger Bands\n",
    "    try:\n",
    "        bb = ta.volatility.BollingerBands(df['close'])\n",
    "        df['BB_MAVG'] = bb.bollinger_mavg()\n",
    "        df['BB_HIGH'] = bb.bollinger_hband()\n",
    "        df['BB_LOW'] = bb.bollinger_lband()\n",
    "        df['BB_WIDTH'] = bb.bollinger_wband()\n",
    "        df['BB_PERCENT'] = bb.bollinger_pband()\n",
    "    except Exception as e:\n",
    "        print(f\"Bollinger Bands Error: {str(e)}\")\n",
    "\n",
    "    # Donchian Channel\n",
    "    try:\n",
    "        df['DC_HIGH'] = ta.volatility.donchian_channel_hband(df['high'], df['low'], df['close'])\n",
    "        df['DC_LOW'] = ta.volatility.donchian_channel_lband(df['high'], df['low'], df['close'])\n",
    "        df['DC_MID'] = (df['DC_HIGH'] + df['DC_LOW']) / 2\n",
    "    except Exception as e:\n",
    "        print(f\"Donchian Channel Error: {str(e)}\")\n",
    "\n",
    "    # ===== VOLUME INDICATORS =====\n",
    "    volume_indicators = [\n",
    "        ('OBV', lambda: ta.volume.on_balance_volume(df['close'], df['volume'])),\n",
    "        ('CMF', lambda: ta.volume.chaikin_money_flow(df['high'], df['low'], df['close'], df['volume'])),\n",
    "        ('MFI', lambda: ta.volume.money_flow_index(df['high'], df['low'], df['close'], df['volume'])),\n",
    "        ('ADI', lambda: ta.volume.acc_dist_index(df['high'], df['low'], df['close'], df['volume'])),\n",
    "        ('EOM', lambda: ta.volume.ease_of_movement(df['high'], df['low'], df['volume'])),\n",
    "        ('VWAP', lambda: ta.volume.volume_weighted_average_price(df['high'], df['low'], df['close'], df['volume'])),\n",
    "        ('FI', lambda: ta.volume.force_index(df['close'], df['volume']))\n",
    "    ]\n",
    "    \n",
    "    for name, func in volume_indicators:\n",
    "        try:\n",
    "            df[name] = func()\n",
    "        except Exception as e:\n",
    "            print(f\"{name} Error: {str(e)}\")\n",
    "\n",
    "    # Volume Moving Averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        try:\n",
    "            df[f'Volume_MA_{window}'] = df['volume'].rolling(window).mean()\n",
    "        except Exception as e:\n",
    "            print(f\"Volume_MA_{window} Error: {str(e)}\")\n",
    "\n",
    "    # ===== CUSTOM INDICATORS =====\n",
    "    try:\n",
    "        # Price * Volume\n",
    "        df['PV'] = df['close'] * df['volume']\n",
    "        \n",
    "        # Returns\n",
    "        df['Daily_Return'] = ta.others.daily_return(df['close'])\n",
    "        df['Cum_Return'] = ta.others.cumulative_return(df['close'])\n",
    "        df['Log_Return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        \n",
    "        # Price Change\n",
    "        df['Price_Change'] = df['close'].diff()\n",
    "        df['Pct_Change'] = df['close'].pct_change(fill_method=None)\n",
    "        \n",
    "        # Volatility\n",
    "        df['HL_Pct'] = (df['high'] - df['low']) / df['low'] * 100\n",
    "        df['OC_Pct'] = (df['close'] - df['open']) / df['open'] * 100\n",
    "    except Exception as e:\n",
    "        print(f\"Custom Indicators Error: {str(e)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# =====================================\n",
    "# üõ† 8. After Download ‚Äî Read CSVs & Process\n",
    "# =====================================\n",
    "\n",
    "def filter_trading_hours(df):\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.set_index('datetime')\n",
    "    df = df.between_time(\"09:15\", \"15:30\").reset_index()\n",
    "    return df\n",
    "\n",
    "def process_options_data(df):\n",
    "    # Validate required columns\n",
    "    if not all(col in df.columns for col in ['strike', 'right']):\n",
    "        raise ValueError(\"DataFrame must contain 'strike' and 'right' columns\")\n",
    "    \n",
    "    # Group and apply indicators without operating on grouping columns\n",
    "    options_df = df.copy()\n",
    "    options_df = options_df.groupby(['strike', 'right'], group_keys=False)\\\n",
    "                           .apply(lambda x: add_all_technical_indicators(x), include_groups=False).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Processed options data with {len(options_df.columns)} indicators\")\n",
    "\n",
    "    return options_df\n",
    "\n",
    "# üßπ Helper to load\n",
    "def load_and_process(file_path):\n",
    "    if not os.path.exists(file_path) or os.stat(file_path).st_size == 0:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file_path} ‚Äî File is missing or empty.\")\n",
    "        return None\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = filter_trading_hours(df)\n",
    "\n",
    "    if 'strike' in df.columns and 'right' in df.columns:\n",
    "        return process_options_data(df)\n",
    "    else:\n",
    "        df = add_all_technical_indicators(df)\n",
    "        print(f\"{file_path} finally has columns: {df.columns}\")\n",
    "    return df\n",
    "\n",
    "# üìÇ File paths\n",
    "equity_file = os.path.join(save_path, \"tcs_equity_data.csv\")\n",
    "future_file = os.path.join(save_path, \"tcs_futures_data.csv\")\n",
    "option_file = os.path.join(save_path, \"tcs_options_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def add_relationship_metadata(combined):\n",
    "\n",
    "    df = combined.copy()\n",
    "\n",
    "    # =================================================\n",
    "    # A. Equity-Futures Relationships\n",
    "    # =================================================\n",
    "    if all(col in df.columns for col in ['equity_close', 'futures_close']):\n",
    "        # Basis calculations\n",
    "        for pt in ['open', 'high', 'low', 'close']:\n",
    "            if f'equity_{pt}' in df.columns and f'futures_{pt}' in df.columns:\n",
    "                df[f'basis_{pt}_absolute'] = df[f'futures_{pt}'] - df[f'equity_{pt}']\n",
    "                df[f'basis_{pt}_relative'] = df[f'basis_{pt}_absolute'] / df[f'equity_{pt}']\n",
    "\n",
    "        # Convergence metrics\n",
    "        if 'basis_close_absolute' in df.columns:\n",
    "            df['basis_close_convergence'] = df['basis_close_absolute'].diff()\n",
    "            df['basis_close_velocity'] = df['basis_close_convergence'].diff()\n",
    "\n",
    "        df['futures_equity_ratio'] = df['futures_close'] / df['equity_close']\n",
    "\n",
    "    # =================================================\n",
    "    # B. Options-Spot Relationships\n",
    "    # =================================================\n",
    "    required_option_cols = ['options_close', 'options_strike_price', 'options_right']\n",
    "    if all(col in df.columns for col in required_option_cols + ['equity_close']):\n",
    "        call_mask = df['options_right'].str.lower() == 'call'\n",
    "\n",
    "        # Moneyness\n",
    "        df['options_moneyness'] = df['equity_close'] / df['options_strike_price']\n",
    "        df['log_moneyness'] = np.log(df['options_strike_price'] / df['equity_close'])\n",
    "\n",
    "        # Intrinsic/extrinsic\n",
    "        df['options_intrinsic_value'] = np.where(\n",
    "            call_mask,\n",
    "            np.maximum(0, df['equity_close'] - df['options_strike_price']),\n",
    "            np.maximum(0, df['options_strike_price'] - df['equity_close'])\n",
    "        )\n",
    "        df['options_extrinsic_value'] = df['options_close'] - df['options_intrinsic_value']\n",
    "        df['options_extrinsic_pct'] = df['options_extrinsic_value'] / df['options_close']\n",
    "\n",
    "    # =================================================\n",
    "    # C. Options-Futures Relationships (NEWLY ADDED BACK)\n",
    "    # =================================================\n",
    "    if all(col in df.columns for col in ['futures_close'] + required_option_cols):\n",
    "        df['options_futures_intrinsic'] = np.where(\n",
    "            call_mask,\n",
    "            np.maximum(0, df['futures_close'] - df['options_strike_price']),\n",
    "            np.maximum(0, df['options_strike_price'] - df['futures_close'])\n",
    "        )\n",
    "        df['options_futures_extrinsic'] = df['options_close'] - df['options_futures_intrinsic']\n",
    "        df['implied_carry_cost'] = (\n",
    "            (df['futures_close'] - df['options_strike_price']) -\n",
    "            (df['options_close'] * np.where(call_mask, 1, -1))\n",
    "        )\n",
    "\n",
    "    # =================================================\n",
    "    # D. Volatility Relationships (Fixed Version)\n",
    "    # =================================================\n",
    "    if all(col in df.columns for col in ['options_close', 'equity_close', 'options_strike_price', 'options_expiry_date']):\n",
    "        try:\n",
    "            # Calculate time to expiry in years\n",
    "            time_to_expiry = ((pd.to_datetime(df['options_expiry_date']) - df['datetime']).dt.days / 365)\n",
    "            time_to_expiry = time_to_expiry.replace(0, np.nan)  # Avoid division by zero\n",
    "\n",
    "            # Calculate simple implied volatility proxy\n",
    "            df['options_implied_volatility'] = (\n",
    "                df['options_close'] /\n",
    "                df['equity_close'] /\n",
    "                np.sqrt(time_to_expiry)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't calculate implied vol: {str(e)}\")\n",
    "\n",
    "    # Keep the realized volatility calculation\n",
    "    if 'equity_close' in df.columns:\n",
    "        log_returns = np.log(df['equity_close'] / df['equity_close'].shift(1))\n",
    "        df['realized_vol_5min'] = log_returns.rolling(12).std() * np.sqrt(252*78)\n",
    "\n",
    "    # =================================================\n",
    "    # E. Technical Indicator Relationships (NEWLY ADDED BACK)\n",
    "    # =================================================\n",
    "    for indicator in ['RSI_14', 'MACD', 'ADX_14']:\n",
    "        if f'equity_{indicator}' in df.columns and f'futures_{indicator}' in df.columns:\n",
    "            df[f'equity_futures_{indicator}_divergence'] = (\n",
    "                df[f'equity_{indicator}'] - df[f'futures_{indicator}'])\n",
    "\n",
    "    # Composite trend strength\n",
    "    if all(col in df.columns for col in ['equity_ADX_14', 'futures_ADX_14']):\n",
    "        df['composite_trend_strength'] = (\n",
    "            0.6*df['equity_ADX_14'] +\n",
    "            0.4*df['futures_ADX_14'])\n",
    "\n",
    "    # =================================================\n",
    "    # F. Time-Based Metrics (KEPT AS REQUESTED)\n",
    "    # =================================================\n",
    "    if 'options_expiry_date' in df.columns:\n",
    "        df['days_to_expiry'] = (pd.to_datetime(df['options_expiry_date']) - df['datetime']).dt.days\n",
    "        if 'options_extrinsic_value' in df.columns:\n",
    "            df['daily_theta'] = df['options_extrinsic_value'] / df['days_to_expiry'].replace(0, np.nan)\n",
    "        df['weekly_expiry'] = df['datetime'].dt.dayofweek == 3  # Thursday expiries\n",
    "\n",
    "    # =================================================\n",
    "    # G. Volume and Liquidity Metrics\n",
    "    # =================================================\n",
    "    # 1. Volume ratios\n",
    "    if all(col in df.columns for col in ['equity_volume', 'futures_volume']):\n",
    "        df['futures_equity_volume_ratio'] = df['futures_volume'] / df['equity_volume']\n",
    "\n",
    "    if 'options_volume' in df.columns and 'equity_volume' in df.columns:\n",
    "        df['options_equity_volume_ratio'] = df['options_volume'] / df['equity_volume']\n",
    "\n",
    "    # 2. Open interest metrics\n",
    "    if 'options_open_interest' in df.columns:\n",
    "        df['options_oi_volume_ratio'] = df['options_open_interest'] / df['options_volume'].replace(0, np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "def enhance_metrics(combined_df):\n",
    "    \"\"\"\n",
    "    Add layers of sophisticated metrics to the combined dataframe\n",
    "    \"\"\"\n",
    "    df = combined_df.copy()\n",
    "\n",
    "    # ==============================================\n",
    "    # A. Price-Based Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    # 1. Extended Basis Calculations\n",
    "    for price_type in ['open', 'high', 'low', 'close']:\n",
    "        if f'equity_{price_type}' in df.columns and f'futures_{price_type}' in df.columns:\n",
    "            df[f'basis_{price_type}_raw'] = df[f'futures_{price_type}'] - df[f'equity_{price_type}']\n",
    "            df[f'basis_{price_type}_pct'] = df[f'basis_{price_type}_raw'] / df[f'equity_{price_type}']\n",
    "\n",
    "    # 2. Price Ratios and Spreads\n",
    "    df['futures_premium'] = df['futures_close'] / df['equity_close'] - 1\n",
    "    df['overnight_gap'] = df['equity_open'] / df['equity_close'].shift(1) - 1\n",
    "\n",
    "    # ==============================================\n",
    "    # B. Options-Specific Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    if 'options_strike_price' in df.columns:\n",
    "        # 1. Advanced Moneyness Metrics\n",
    "        df['log_moneyness'] = np.log(df['equity_close'] / df['options_strike_price'])\n",
    "        df['forward_moneyness'] = df['futures_close'] / df['options_strike_price'] if 'futures_close' in df.columns else np.nan\n",
    "\n",
    "        # 2. Synthetic Greeks (simplified)\n",
    "        required_cols = ['log_moneyness', 'options_right', 'options_implied_volatility', 'equity_close', 'options_expiry_date', 'datetime']\n",
    "        if all(col in df.columns for col in required_cols):\n",
    "\n",
    "          # Time to expiry in years\n",
    "          df['time_to_expiry'] = (pd.to_datetime(df['options_expiry_date']) - df['datetime']).dt.days / 365\n",
    "          df['time_to_expiry'] = df['time_to_expiry'].clip(lower=1e-6)  # prevent division by zero\n",
    "\n",
    "          # Convenience variables\n",
    "          d1 = df['log_moneyness'] / (df['options_implied_volatility'] * np.sqrt(df['time_to_expiry']))\n",
    "          d2 = d1 - df['options_implied_volatility'] * np.sqrt(df['time_to_expiry'])\n",
    "\n",
    "          # Delta Proxy\n",
    "          df['delta_proxy'] = np.where(\n",
    "              df['options_right'] == 'call',\n",
    "              stats.norm.cdf(d1),\n",
    "              stats.norm.cdf(d1) - 1\n",
    "              )\n",
    "\n",
    "          # Gamma Proxy\n",
    "          df['gamma_proxy'] = stats.norm.pdf(d1) / (df['equity_close'] * df['options_implied_volatility'] * np.sqrt(df['time_to_expiry']))\n",
    "\n",
    "          # Vega Proxy (scaled to 1% vol change)\n",
    "          df['vega_proxy'] = df['equity_close'] * stats.norm.pdf(d1) * np.sqrt(df['time_to_expiry']) / 100\n",
    "\n",
    "        # 3. Probability Metrics\n",
    "        df['itm_probability'] = np.where(df['options_right'] == 'call',\n",
    "                                       stats.norm.cdf(df['log_moneyness']),\n",
    "                                       stats.norm.cdf(-df['log_moneyness']))\n",
    "\n",
    "        df['otm_probability'] = np.where(df['options_right'] == 'call',\n",
    "                                        stats.norm.cdf(-df['log_moneyness']),\n",
    "                                        stats.norm.cdf(df['log_moneyness']))\n",
    "\n",
    "        df['atm_probability'] = 2 * np.abs(df['log_moneyness']) - df['itm_probability'] - df['otm_probability']\n",
    "\n",
    "    # ==============================================\n",
    "    # C. Volume and Liquidity Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    # 1. Volume Ratios\n",
    "    for inst in ['futures', 'options']:\n",
    "        if f'{inst}_volume' in df.columns:\n",
    "            df[f'{inst}_volume_ratio'] = df[f'{inst}_volume'] / df['equity_volume']\n",
    "            df[f'{inst}_volatility_ratio'] = df[f'{inst}_volume'] / df[f'{inst}_close'].rolling(20).std()\n",
    "\n",
    "    # 2. Liquidity Proxies\n",
    "    if 'options_open_interest' in df.columns:\n",
    "        df['options_liquidity_score'] = (df['options_volume'] * df['options_close']) / df['options_open_interest']\n",
    "\n",
    "    # ==============================================\n",
    "    # D. Technical Composite Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    # 1. Divergence Scores\n",
    "    for indicator in ['RSI_14', 'MACD', 'ATR_14']:\n",
    "        for pair in [('equity', 'futures'), ('equity', 'options')]:\n",
    "            if f'{pair[0]}_{indicator}' in df.columns and f'{pair[1]}_{indicator}' in df.columns:\n",
    "                df[f'{pair[0]}_{pair[1]}_{indicator}_divergence'] = (\n",
    "                    df[f'{pair[0]}_{indicator}'] - df[f'{pair[1]}_{indicator}'])\n",
    "\n",
    "    # 2. Multi-Instrument Signals\n",
    "    df['composite_trend_strength'] = (\n",
    "        0.4*df['equity_ADX_14'] +\n",
    "        0.3*df['futures_ADX_14'] +\n",
    "        0.3*df['options_implied_volatility'])\n",
    "\n",
    "    # 3. Market regime classification\n",
    "    if all(col in df.columns for col in ['equity_RSI_14', 'futures_RSI_14']):\n",
    "        conditions = [\n",
    "            (df['equity_RSI_14'] > 70) & (df['futures_RSI_14'] > 70),\n",
    "            (df['equity_RSI_14'] < 30) & (df['futures_RSI_14'] < 30)\n",
    "        ]\n",
    "        df['market_regime'] = np.select(conditions, ['overbought', 'oversold'], default='neutral')\n",
    "\n",
    "    # ==============================================\n",
    "    # E. Time-Based Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    if 'options_expiry_date' in df.columns:\n",
    "        # 1. Time Decay Metrics\n",
    "        df['days_to_expiry'] = (pd.to_datetime(df['options_expiry_date']) - df['datetime']).dt.days\n",
    "        df['theta_proxy'] = df['options_extrinsic_value'] / np.sqrt(df['days_to_expiry'])\n",
    "\n",
    "        # 2. Expiry-Cluster Effects\n",
    "        df['weekly_expiry'] = df['datetime'].dt.dayofweek == 3  # Thursday expiries\n",
    "\n",
    "    # ==============================================\n",
    "    # F. Risk Metrics\n",
    "    # ==============================================\n",
    "\n",
    "    # 1. Value-at-Risk Proxies\n",
    "    for inst in ['equity', 'futures', 'options']:\n",
    "        if f'{inst}_close' in df.columns:\n",
    "            returns = df[f'{inst}_close'].pct_change(fill_method=None)\n",
    "            df[f'{inst}_var_95'] = returns.rolling(20).apply(lambda x: np.percentile(x.dropna(), 5))\n",
    "\n",
    "    # 2. Correlation Metrics\n",
    "    if all(col in df.columns for col in ['equity_close', 'futures_close', 'options_close']):\n",
    "        df['spot_futures_correlation'] = df['equity_close'].rolling(20).corr(df['futures_close'])\n",
    "\n",
    "    # 3. Calculate cross-correlations (FIXED for lookahead bias)\n",
    "    # FIXED: Use rolling correlation to avoid lookahead bias\n",
    "    # Previous line was problematic: shift(1).corr() doesn't work as intended\n",
    "    df['futures_equity_correlation_20'] = df['futures_close'].rolling(20).corr(df['equity_close'])\n",
    "    \n",
    "    # If you need lag correlation, use proper rolling lag correlation\n",
    "    df['futures_equity_lag1_correlation_20'] = df['futures_close'].shift(1).rolling(20).corr(df['equity_close'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def combine_all_data(equity_df, futures_df, options_df):\n",
    "\n",
    "    from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "\n",
    "    # Create copies to avoid modifying originals\n",
    "    equity = equity_df.copy()\n",
    "    futures = futures_df.copy()\n",
    "    options = options_df.copy()\n",
    "\n",
    "    # =====================================================================\n",
    "    # 1. Standardize datetime across all datasets\n",
    "    # =====================================================================\n",
    "    for df in [equity, futures, options]:\n",
    "        if 'datetime' in df.columns and not is_datetime(df['datetime']):\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # =====================================================================\n",
    "    # 2. Process Equity Data (Base Timeline)\n",
    "    # =====================================================================\n",
    "    equity = equity.add_prefix('equity_')\n",
    "    equity.rename(columns={'equity_datetime': 'datetime'}, inplace=True)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 3. Process Futures Data (1:1 with Equity)\n",
    "    # =====================================================================\n",
    "    futures = futures.add_prefix('futures_')\n",
    "    futures.rename(columns={'futures_datetime': 'datetime'}, inplace=True)\n",
    "\n",
    "    # Keep only the nearest expiry contract (most liquid)\n",
    "    if 'futures_expiry_date' in futures.columns:\n",
    "        nearest_expiry = futures['futures_expiry_date'].min()\n",
    "        futures = futures[futures['futures_expiry_date'] == nearest_expiry]\n",
    "\n",
    "    # =====================================================================\n",
    "    # 4. Process Options Data (Complex - Many-to-One with Equity)\n",
    "    # =====================================================================\n",
    "    options = options.add_prefix('options_')\n",
    "    options.rename(columns={'options_datetime': 'datetime'}, inplace=True)\n",
    "\n",
    "    # Extract option type (call/put) from symbol if right column doesn't exist\n",
    "    if 'options_right' not in options.columns and 'options_symbol' in options.columns:\n",
    "        options['options_right'] = options['options_symbol'].str[-1].str.lower().map({'c':'call', 'p':'put'})\n",
    "    elif 'options_strike_price' in options.columns and 'options_symbol' in options.columns:\n",
    "        # Alternative method to determine call/put if right isn't available\n",
    "        options['options_right'] = options['options_symbol'].str.extract(r'([CP])$')[0].str.lower().map({'c':'call', 'p':'put'})\n",
    "    else:\n",
    "        raise ValueError(\"Cannot determine option type (call/put) - missing both 'right' and 'symbol' columns\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # 5. Multi-Level Combination Strategy\n",
    "    # =====================================================================\n",
    "    # First merge equity and futures (1:1 relationship)\n",
    "    combined = pd.merge(\n",
    "        equity,\n",
    "        futures,\n",
    "        on='datetime',\n",
    "        how='outer',\n",
    "        suffixes=('', '_futures')\n",
    "    )\n",
    "\n",
    "    # Then merge with options (many-to-one relationship)\n",
    "    combined = pd.merge(\n",
    "        combined,\n",
    "        options,\n",
    "        on='datetime',\n",
    "        how='outer',\n",
    "        suffixes=('', '_options')\n",
    "    )\n",
    "\n",
    "    # =====================================================================\n",
    "    # 6. Column Organization and Cleanup\n",
    "    # =====================================================================\n",
    "    # Reorder columns logically\n",
    "    base_cols = ['datetime']\n",
    "\n",
    "    equity_cols = [c for c in combined.columns\n",
    "                  if c.startswith('equity_') and c not in base_cols]\n",
    "\n",
    "    futures_cols = [c for c in combined.columns\n",
    "                  if c.startswith('futures_') and c not in base_cols]\n",
    "\n",
    "    options_cols = [c for c in combined.columns\n",
    "                  if c.startswith('options_') and c not in base_cols]\n",
    "\n",
    "    # Final column order\n",
    "    column_order = (\n",
    "        base_cols +\n",
    "        sorted(equity_cols) +\n",
    "        sorted(futures_cols) +\n",
    "        sorted(options_cols)\n",
    "    )\n",
    "\n",
    "    combined = combined[column_order]\n",
    "\n",
    "    # =====================================================================\n",
    "    # 7. Add Relationship Metadata\n",
    "    # =====================================================================\n",
    "    combined = add_relationship_metadata(combined)\n",
    "\n",
    "    # Add expiration countdown for derivatives\n",
    "    for col in ['futures_expiry_date', 'options_expiry_date']:\n",
    "        if col in combined.columns:\n",
    "            combined[f'{col}_days_remaining'] = (\n",
    "                pd.to_datetime(combined[col]) - combined['datetime']\n",
    "            ).dt.days\n",
    "\n",
    "    return combined\n",
    "\n",
    "try:\n",
    "    master_df = combine_all_data(equity_df, future_df, option_df)\n",
    "    print(\"‚úÖ Successfully combined data!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error combining data: {str(e)}\")\n",
    "\n",
    "# Then enhance with additional metrics\n",
    "enhanced_df = enhance_metrics(master_df)\n",
    "print(\"‚úÖ Successfully enhanced data!.\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure datetime is index and sorted\n",
    "enhanced_df.index = pd.to_datetime(enhanced_df.index)\n",
    "enhanced_df = enhanced_df.sort_index()\n",
    "\n",
    "# Replace infinite values with NaNs, then fill\n",
    "enhanced_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "enhanced_df.ffill(inplace=True)\n",
    "enhanced_df.bfill(inplace=True)\n",
    "\n",
    "# Identify numeric columns only (skip string/object types like stock_code)\n",
    "numeric_cols = enhanced_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Filter equity/futures/options to only numeric indicators\n",
    "equity_cols = [col for col in numeric_cols if col.startswith(\"equity_\")]\n",
    "futures_cols = [col for col in numeric_cols if col.startswith(\"futures_\")]\n",
    "options_cols = [col for col in numeric_cols if col.startswith(\"options_\")]\n",
    "\n",
    "# Strip prefixes to find matching indicator names\n",
    "equity_indicators = set(col.replace(\"equity_\", \"\") for col in equity_cols)\n",
    "futures_indicators = set(col.replace(\"futures_\", \"\") for col in futures_cols)\n",
    "options_indicators = set(col.replace(\"options_\", \"\") for col in options_cols)\n",
    "\n",
    "# Common indicators\n",
    "shared_futures = equity_indicators & futures_indicators\n",
    "shared_options = equity_indicators & options_indicators\n",
    "\n",
    "# Rolling windows\n",
    "rolling_windows = [5, 10, 20, 50]\n",
    "\n",
    "# Function to calculate rolling correlation (FIXED for lookahead bias)\n",
    "def compute_rolling_corr_fixed(df, col1, col2, window):\n",
    "    \"\"\"\n",
    "    Calculate rolling correlation ensuring no lookahead bias.\n",
    "    Uses only historical data for each timestamp.\n",
    "    \"\"\"\n",
    "    # The default pandas rolling().corr() is correct - it uses only past data\n",
    "    # But we add explicit min_periods to ensure robust calculation\n",
    "    return df[col1].rolling(window=window, min_periods=window//2).corr(df[col2])\n",
    "\n",
    "# Collect all new columns in a dictionary first\n",
    "new_corr_cols = {}\n",
    "\n",
    "print(\"üõ°Ô∏è Computing rolling correlations with lookahead bias protection...\")\n",
    "\n",
    "for indicator in shared_futures:\n",
    "    eq_col = f\"equity_{indicator}\"\n",
    "    fut_col = f\"futures_{indicator}\"\n",
    "    if eq_col in enhanced_df.columns and fut_col in enhanced_df.columns:\n",
    "        for win in rolling_windows:\n",
    "            new_col = f\"corr_equity_futures_{indicator}_win{win}\"\n",
    "            new_corr_cols[new_col] = compute_rolling_corr_fixed(enhanced_df, eq_col, fut_col, win)\n",
    "\n",
    "for indicator in shared_options:\n",
    "    eq_col = f\"equity_{indicator}\"\n",
    "    opt_col = f\"options_{indicator}\"\n",
    "    if eq_col in enhanced_df.columns and opt_col in enhanced_df.columns:\n",
    "        for win in rolling_windows:\n",
    "            new_col = f\"corr_equity_options_{indicator}_win{win}\"\n",
    "            new_corr_cols[new_col] = compute_rolling_corr_fixed(enhanced_df, eq_col, opt_col, win)\n",
    "\n",
    "# Efficiently join all new columns at once\n",
    "enhanced_df = pd.concat([enhanced_df, pd.DataFrame(new_corr_cols, index=enhanced_df.index)], axis=1)\n",
    "\n",
    "print(\"‚úÖ Cleaned non-numeric columns and added rolling correlation features efficiently.\")\n",
    "\n",
    "def add_all_returns(df, price_columns, rolling_windows=[3, 5, 10], risk_window=20):\n",
    "    df = df.copy()  # Prevent modifying original\n",
    "\n",
    "    for col in price_columns:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Simple Return\n",
    "        df[f'{col}_return'] = df[col].pct_change()\n",
    "\n",
    "        # Log Return ‚Äî add safe check for zero/negative\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            log_return = np.log(df[col] / df[col].shift(1))\n",
    "            log_return.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            df[f'{col}_log_return'] = log_return\n",
    "\n",
    "        # Rolling Returns\n",
    "        for w in rolling_windows:\n",
    "            df[f'{col}_rolling_return_{w}'] = df[col].pct_change(periods=w)\n",
    "\n",
    "        # Risk-adjusted return (Sharpe proxy)\n",
    "        returns = df[col].pct_change()\n",
    "        rolling_mean = returns.rolling(risk_window).mean()\n",
    "        rolling_std = returns.rolling(risk_window).std()\n",
    "        sharpe_like = rolling_mean / rolling_std\n",
    "        sharpe_like.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df[f'{col}_risk_adjusted_return'] = sharpe_like\n",
    "\n",
    "    # Optionally clean up all NaNs and Infs\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.bfill(inplace=True)\n",
    "    df.ffill(inplace=True)  # Or use df.fillna(method='bfill'), etc.\n",
    "\n",
    "    return df\n",
    "\n",
    "price_cols = [\n",
    "    'equity_close', 'equity_high', 'equity_low', 'equity_open', 'equity_volume',\n",
    "    'futures_close', 'futures_high', 'futures_low', 'futures_open', 'futures_volume',\n",
    "    'options_close', 'options_high', 'options_low', 'options_open', 'options_volume'\n",
    "]\n",
    "enhanced_df = add_all_returns(enhanced_df, price_cols)\n",
    "print(f\"‚úÖ Successfully added returns!\")\n",
    "\n",
    "# --- SETTINGS ---\n",
    "correlation_threshold = 0.95\n",
    "target_time_windows = [1, 5, 10, 15, 30]\n",
    "target_thresholds = [0.005, 0.0075, 0.01, 0.015, 0.02, 0.25, 0.03, 0.05, 0.075, 0.10, 0.15, 0.20]  # 0.5%, 1%, 2%\n",
    "\n",
    "# --- 1. CORRELATION FILTERING ---\n",
    "def correlation_filter(df, columns, threshold):\n",
    "    corr = df[columns].corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    drop_cols = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    return [col for col in columns if col not in drop_cols], drop_cols\n",
    "\n",
    "filtered_price_cols, dropped_price_cols = correlation_filter(enhanced_df, price_cols, correlation_threshold)\n",
    "print(\"Dropped highly correlated price columns:\", dropped_price_cols)\n",
    "\n",
    "# --- 2. TARGET GENERATION (FIXED FOR LOOKAHEAD BIAS) ---\n",
    "def generate_price_targets_fixed(df, columns, time_windows, thresholds):\n",
    "    \"\"\"\n",
    "    Generate price targets WITHOUT lookahead bias.\n",
    "    \n",
    "    CRITICAL FIX: This version removes the problematic shift(-win) operation\n",
    "    that was causing lookahead bias in the original implementation.\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Generating targets with lookahead bias protection...\")\n",
    "    \n",
    "    all_new_cols = []\n",
    "\n",
    "    for col in columns:\n",
    "        for win in time_windows:\n",
    "            # CORRECT: Calculate future returns for target generation\n",
    "            # This is acceptable for targets, but these must not be used as features\n",
    "            current_prices = df[col]\n",
    "            future_prices = df[col].shift(-win)  # OK for targets only\n",
    "            \n",
    "            # Calculate returns properly\n",
    "            future_return = (future_prices - current_prices) / current_prices\n",
    "            \n",
    "            # Create target column names with clear indication these are targets\n",
    "            target_ret_col = f'{col}_TARGET_ret_{win}periods'\n",
    "            target_logret_col = f'{col}_TARGET_logret_{win}periods'\n",
    "\n",
    "            # Clip values to prevent extreme outliers\n",
    "            future_return_clipped = future_return.replace([np.inf, -np.inf], np.nan).clip(lower=-0.999)\n",
    "            log_return = np.log1p(future_return_clipped.fillna(0))\n",
    "\n",
    "            new_cols = {\n",
    "                target_ret_col: future_return,\n",
    "                target_logret_col: log_return\n",
    "            }\n",
    "\n",
    "            # Classification labels (these are also targets)\n",
    "            for thresh in thresholds:\n",
    "                thresh_str = str(thresh).replace('.', '_')\n",
    "                up = (future_return > thresh).astype(int)\n",
    "                down = (future_return < -thresh).astype(int)\n",
    "                neutral = ((future_return <= thresh) & (future_return >= -thresh)).astype(int)\n",
    "\n",
    "                new_cols[f'{col}_TARGET_up_{win}p_{thresh_str}'] = up\n",
    "                new_cols[f'{col}_TARGET_down_{win}p_{thresh_str}'] = down\n",
    "                new_cols[f'{col}_TARGET_neutral_{win}p_{thresh_str}'] = neutral\n",
    "\n",
    "            # Append to collection\n",
    "            all_new_cols.append(pd.DataFrame(new_cols, index=df.index))\n",
    "\n",
    "    # Concatenate all new columns\n",
    "    if all_new_cols:\n",
    "        new_features_df = pd.concat(all_new_cols, axis=1)\n",
    "        df = pd.concat([df, new_features_df], axis=1)\n",
    "\n",
    "    # CRITICAL: Remove the last max(time_windows) rows as they will have NaN targets\n",
    "    max_window = max(time_windows)\n",
    "    original_length = len(df)\n",
    "    \n",
    "    if len(df) > max_window:\n",
    "        df = df[:-max_window].copy()\n",
    "        print(f\"üõ°Ô∏è LOOKAHEAD BIAS PROTECTION: Removed last {max_window} rows\")\n",
    "        print(f\"   Dataset length: {original_length} ‚Üí {len(df)}\")\n",
    "        print(f\"   This prevents using future data in training!\")\n",
    "\n",
    "    # Clean up infinite values and NaNs\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # For target columns, keep NaN values (don't fill with 0)\n",
    "    # For feature columns, can fill with appropriate methods\n",
    "    target_cols = [col for col in df.columns if 'TARGET' in col]\n",
    "    feature_cols = [col for col in df.columns if 'TARGET' not in col]\n",
    "    \n",
    "    # Fill feature columns only\n",
    "    if feature_cols:\n",
    "        df[feature_cols] = df[feature_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    df = df.copy()  # defragment\n",
    "\n",
    "    print(f\"‚úÖ Target generation completed with {len(target_cols)} target columns\")\n",
    "    print(f\"‚ö†Ô∏è  Remember: TARGET columns are for prediction only, not features!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "enhanced_df = generate_price_targets_fixed(enhanced_df, filtered_price_cols, target_time_windows, target_thresholds)\n",
    "\n",
    "print(f\"‚úÖ Successfully Added Bias-Free Price Targets!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
