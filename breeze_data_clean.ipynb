{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b96bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üõ† Enhanced Modular Data Collection\n",
    "# =====================================\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Import our enhanced utilities\n",
    "from app_config import Config\n",
    "from enhanced_breeze_utils import EnhancedBreezeDataManager, OptionChainAnalyzer\n",
    "from data_processing_utils import TechnicalIndicatorProcessor, OptionsDataProcessor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =====================================\n",
    "# üîí Data Validation Utilities\n",
    "# =====================================\n",
    "\n",
    "def validate_dataframe_structure(df: pd.DataFrame, required_columns: list, \n",
    "                               operation_name: str = \"operation\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate DataFrame structure and handle missing columns safely.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        required_columns: List of required column names\n",
    "        operation_name: Name of operation for error messages\n",
    "    \n",
    "    Returns:\n",
    "        Validated DataFrame with required columns\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If DataFrame is empty or critical columns are missing\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(f\"DataFrame is empty for {operation_name}\")\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logger.warning(f\"Missing columns for {operation_name}: {missing_columns}\")\n",
    "        # Add missing columns with NaN values\n",
    "        for col in missing_columns:\n",
    "            df[col] = np.nan\n",
    "            logger.info(f\"Added missing column '{col}' with NaN values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensure_numeric_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure specified columns are numeric, converting if necessary.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        columns: List of column names to ensure are numeric\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with numeric columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "            \n",
    "            # Log conversion issues\n",
    "            nan_count = df_copy[col].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                logger.warning(f\"Column '{col}': {nan_count} values converted to NaN during numeric conversion\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def validate_datetime_column(df: pd.DataFrame, datetime_col: str = 'datetime') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate and ensure datetime column exists and is properly formatted.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        datetime_col: Name of datetime column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with validated datetime column\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if datetime_col not in df_copy.columns:\n",
    "        logger.error(f\"Datetime column '{datetime_col}' not found in DataFrame\")\n",
    "        # Try common datetime column names\n",
    "        datetime_candidates = ['timestamp', 'date', 'time', 'dt']\n",
    "        for candidate in datetime_candidates:\n",
    "            if candidate in df_copy.columns:\n",
    "                logger.info(f\"Using '{candidate}' as datetime column instead\")\n",
    "                df_copy[datetime_col] = df_copy[candidate]\n",
    "                break\n",
    "        else:\n",
    "            # Create a datetime index if none found\n",
    "            logger.warning(f\"No datetime column found, creating sequential datetime index\")\n",
    "            df_copy[datetime_col] = pd.date_range(start='2024-01-01', periods=len(df_copy), freq='1T')\n",
    "    \n",
    "    # Ensure datetime format\n",
    "    try:\n",
    "        df_copy[datetime_col] = pd.to_datetime(df_copy[datetime_col], errors='coerce')\n",
    "        invalid_dates = df_copy[datetime_col].isna().sum()\n",
    "        if invalid_dates > 0:\n",
    "            logger.warning(f\"Found {invalid_dates} invalid datetime entries, filling with interpolation\")\n",
    "            df_copy[datetime_col] = df_copy[datetime_col].interpolate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting datetime column: {e}\")\n",
    "        raise ValueError(f\"Cannot convert '{datetime_col}' to datetime format\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def safe_column_operation(df: pd.DataFrame, operation_func, required_columns: list, \n",
    "                         operation_name: str = \"operation\", default_value=np.nan):\n",
    "    \"\"\"\n",
    "    Safely perform operations that depend on specific columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        operation_func: Function to execute if columns exist\n",
    "        required_columns: List of required columns\n",
    "        operation_name: Name of operation for logging\n",
    "        default_value: Default value to return if operation fails\n",
    "    \n",
    "    Returns:\n",
    "        Result of operation_func or default_value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if all required columns exist\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            logger.warning(f\"Skipping {operation_name}: missing columns {missing_cols}\")\n",
    "            return default_value\n",
    "        \n",
    "        # Check if columns have sufficient non-null data\n",
    "        for col in required_columns:\n",
    "            if df[col].isna().all():\n",
    "                logger.warning(f\"Skipping {operation_name}: column '{col}' is all NaN\")\n",
    "                return default_value\n",
    "        \n",
    "        return operation_func()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {operation_name}: {e}\")\n",
    "        return default_value\n",
    "\n",
    "# =====================================\n",
    "# üõ† Initialize Enhanced Data Manager\n",
    "# =====================================\n",
    "\n",
    "try:\n",
    "    # Initialize configuration and enhanced data manager\n",
    "    config = Config()\n",
    "    data_manager = EnhancedBreezeDataManager()\n",
    "    \n",
    "    # Initialize processing utilities\n",
    "    indicator_processor = TechnicalIndicatorProcessor()\n",
    "    options_processor = OptionsDataProcessor()\n",
    "    option_analyzer = OptionChainAnalyzer()\n",
    "    \n",
    "    # Set up Google Drive if in Colab environment\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"‚úÖ Google Drive mounted\")\n",
    "    except ImportError:\n",
    "        logger.info(\"‚ÑπÔ∏è Not in Colab environment, skipping Google Drive mount\")\n",
    "    \n",
    "    # Authenticate with enhanced retry logic\n",
    "    auth_result = data_manager.authenticate()\n",
    "    if auth_result.success:\n",
    "        logger.info(\"‚úÖ Breeze API authenticated successfully\")\n",
    "        breeze = data_manager.breeze\n",
    "    else:\n",
    "        logger.error(f\"‚ùå Authentication failed: {auth_result.error_message}\")\n",
    "        raise Exception(f\"Authentication failed: {auth_result.error_message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Critical initialization error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"‚úÖ All modules and enhanced utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üõ† Parameter Setup with Validation\n",
    "# =====================================\n",
    "\n",
    "from enhanced_breeze_utils import MarketDataRequest\n",
    "from data_processing_utils import ValidationError\n",
    "\n",
    "def setup_trading_parameters():\n",
    "    \"\"\"Setup and validate trading parameters with proper error handling\"\"\"\n",
    "    try:\n",
    "        # Basic parameters\n",
    "        stock_name = \"TCS\"\n",
    "        interval = \"5minute\"\n",
    "        \n",
    "        # Get trading dates using enhanced utilities\n",
    "        date_result = data_manager.get_trading_dates(days_back=30)\n",
    "        if not date_result.success:\n",
    "            raise ValidationError(f\"Failed to get trading dates: {date_result.error_message}\")\n",
    "        \n",
    "        from_date = date_result.data['from_date']\n",
    "        to_date = date_result.data['to_date']\n",
    "        \n",
    "        logger.info(f\"üìÖ Trading period: {from_date} to {to_date}\")\n",
    "        \n",
    "        # Get current LTP with enhanced error handling\n",
    "        ltp_result = data_manager.get_live_price(stock_name, \"NSE\")\n",
    "        if not ltp_result.success:\n",
    "            raise ValidationError(f\"Failed to get LTP: {ltp_result.error_message}\")\n",
    "        \n",
    "        ltp = ltp_result.data['ltp']\n",
    "        logger.info(f\"üì¶ Current LTP for {stock_name}: {ltp}\")\n",
    "        \n",
    "        # Get valid expiry using enhanced option analyzer\n",
    "        expiry_result = option_analyzer.get_next_valid_expiry(stock_name)\n",
    "        if not expiry_result.success:\n",
    "            raise ValidationError(f\"Failed to get expiry: {expiry_result.error_message}\")\n",
    "        \n",
    "        expiry_date = expiry_result.data['expiry_date']\n",
    "        logger.info(f\"üìå Using expiry: {expiry_date}\")\n",
    "        \n",
    "        # Create structured request object\n",
    "        request = MarketDataRequest(\n",
    "            stock_code=stock_name,\n",
    "            exchange_code=\"NSE\",\n",
    "            interval=interval,\n",
    "            from_date=from_date,\n",
    "            to_date=to_date,\n",
    "            expiry_date=expiry_date,\n",
    "            current_price=ltp\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Parameter setup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Setup parameters\n",
    "market_request = setup_trading_parameters()\n",
    "logger.info(\"‚úÖ Parameters setup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üìà Fetch Equity Data\n",
    "# =====================================\n",
    "\n",
    "def fetch_equity_data(request):\n",
    "    \"\"\"Fetch equity data with comprehensive error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üìä Fetching equity data for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced data manager for equity data\n",
    "        equity_result = data_manager.fetch_historical_data(\n",
    "            stock_code=request.stock_code,\n",
    "            exchange_code=request.exchange_code,\n",
    "            product_type=\"cash\",\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date\n",
    "        )\n",
    "        \n",
    "        if not equity_result.success:\n",
    "            raise ValidationError(f\"Equity data fetch failed: {equity_result.error_message}\")\n",
    "        \n",
    "        equity_df = equity_result.data\n",
    "        \n",
    "        # Validate data structure and types\n",
    "        logger.info(\"üîí Validating equity data structure...\")\n",
    "        required_equity_columns = ['open', 'high', 'low', 'close', 'volume', 'datetime']\n",
    "        equity_df = validate_dataframe_structure(equity_df, required_equity_columns, \"equity data processing\")\n",
    "        \n",
    "        # Ensure datetime column is properly formatted\n",
    "        equity_df = validate_datetime_column(equity_df)\n",
    "        \n",
    "        # Ensure numeric columns are properly typed\n",
    "        numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        equity_df = ensure_numeric_columns(equity_df, numeric_columns)\n",
    "        \n",
    "        # Validate OHLC logic\n",
    "        def validate_ohlc_logic():\n",
    "            invalid_ohlc = equity_df[\n",
    "                (equity_df['high'] < equity_df['low']) |\n",
    "                (equity_df['high'] < equity_df['open']) |\n",
    "                (equity_df['high'] < equity_df['close']) |\n",
    "                (equity_df['low'] > equity_df['open']) |\n",
    "                (equity_df['low'] > equity_df['close'])\n",
    "            ]\n",
    "            if not invalid_ohlc.empty:\n",
    "                logger.warning(f\"Found {len(invalid_ohlc)} rows with invalid OHLC logic\")\n",
    "                # Fix invalid OHLC by setting high/low to reasonable values\n",
    "                equity_df.loc[invalid_ohlc.index, 'high'] = equity_df.loc[invalid_ohlc.index, ['open', 'close']].max(axis=1)\n",
    "                equity_df.loc[invalid_ohlc.index, 'low'] = equity_df.loc[invalid_ohlc.index, ['open', 'close']].min(axis=1)\n",
    "                logger.info(\"Fixed invalid OHLC values\")\n",
    "        \n",
    "        safe_column_operation(equity_df, validate_ohlc_logic, numeric_columns, \"OHLC validation\")\n",
    "        \n",
    "        # Process with technical indicators using enhanced processor\n",
    "        logger.info(\"üìä Processing technical indicators...\")\n",
    "        processing_result = indicator_processor.process_dataframe(\n",
    "            equity_df,\n",
    "            add_all_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Technical indicator processing had issues: {processing_result.error_message}\")\n",
    "            # Continue with raw data if indicator processing fails\n",
    "            processed_df = equity_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "        \n",
    "        # Final validation of processed data\n",
    "        if processed_df.empty:\n",
    "            raise ValueError(\"Processed equity data is empty\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Equity data validated: {len(processed_df)} records, {len(processed_df.columns)} features\")\n",
    "        \n",
    "        # Save with metadata\n",
    "        def calculate_indicators_count():\n",
    "            base_columns = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "            return len([c for c in processed_df.columns if c not in base_columns])\n",
    "        \n",
    "        indicators_count = safe_column_operation(\n",
    "            processed_df, calculate_indicators_count, [], \"indicators count calculation\", 0\n",
    "        )\n",
    "        \n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_equity_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"equity\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"interval\": request.interval,\n",
    "                \"indicators_count\": indicators_count,\n",
    "                \"data_validation\": \"passed\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"‚úÖ Equity data saved: {len(processed_df)} records with {len(processed_df.columns)} features\")\n",
    "        else:\n",
    "            logger.warning(f\"Save failed: {save_result.error_message}\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Equity data fetch failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Fetch equity data with validation\n",
    "equity_df = fetch_equity_data(market_request)\n",
    "logger.info(f\"üìà Equity data shape: {equity_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f923f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üìä Fetch Futures Data\n",
    "# =====================================\n",
    "\n",
    "def fetch_futures_data(request):\n",
    "    \"\"\"Fetch futures data with comprehensive error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üìä Fetching futures data for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced data manager for futures data\n",
    "        futures_result = data_manager.fetch_historical_data(\n",
    "            stock_code=request.stock_code,\n",
    "            exchange_code=\"NFO\",\n",
    "            product_type=\"futures\",\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date,\n",
    "            expiry_date=request.expiry_date\n",
    "        )\n",
    "        \n",
    "        if not futures_result.success:\n",
    "            logger.warning(f\"Futures data fetch failed: {futures_result.error_message}\")\n",
    "            return None  # Return None instead of failing completely\n",
    "        \n",
    "        futures_df = futures_result.data\n",
    "        \n",
    "        # Validate data structure and types\n",
    "        logger.info(\"üîí Validating futures data structure...\")\n",
    "        required_futures_columns = ['open', 'high', 'low', 'close', 'volume', 'datetime']\n",
    "        futures_df = validate_dataframe_structure(futures_df, required_futures_columns, \"futures data processing\")\n",
    "        \n",
    "        # Ensure datetime column is properly formatted\n",
    "        futures_df = validate_datetime_column(futures_df)\n",
    "        \n",
    "        # Ensure numeric columns are properly typed\n",
    "        numeric_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        futures_df = ensure_numeric_columns(futures_df, numeric_columns)\n",
    "        \n",
    "        # Check for sufficient data\n",
    "        if len(futures_df) < 10:\n",
    "            logger.warning(f\"Limited futures data: only {len(futures_df)} records\")\n",
    "        \n",
    "        # Validate futures-specific data\n",
    "        def validate_futures_data():\n",
    "            # Check for reasonable volume values\n",
    "            if 'volume' in futures_df.columns:\n",
    "                zero_volume_count = (futures_df['volume'] == 0).sum()\n",
    "                if zero_volume_count > len(futures_df) * 0.5:\n",
    "                    logger.warning(f\"High zero volume count in futures data: {zero_volume_count}/{len(futures_df)}\")\n",
    "            \n",
    "            # Check for price continuity\n",
    "            if 'close' in futures_df.columns:\n",
    "                price_jumps = futures_df['close'].pct_change().abs()\n",
    "                extreme_jumps = (price_jumps > 0.1).sum()  # >10% price jumps\n",
    "                if extreme_jumps > 0:\n",
    "                    logger.warning(f\"Found {extreme_jumps} extreme price jumps in futures data\")\n",
    "        \n",
    "        safe_column_operation(futures_df, validate_futures_data, numeric_columns, \"futures data validation\")\n",
    "        \n",
    "        # Process with technical indicators\n",
    "        logger.info(\"üìä Processing futures technical indicators...\")\n",
    "        processing_result = indicator_processor.process_dataframe(\n",
    "            futures_df,\n",
    "            add_all_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Futures technical indicator processing failed: {processing_result.error_message}\")\n",
    "            processed_df = futures_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "        \n",
    "        # Final validation\n",
    "        if processed_df.empty:\n",
    "            logger.warning(\"Processed futures data is empty\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"‚úÖ Futures data validated: {len(processed_df)} records, {len(processed_df.columns)} features\")\n",
    "        \n",
    "        # Save with metadata\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_futures_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"futures\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"expiry_date\": request.expiry_date,\n",
    "                \"interval\": request.interval,\n",
    "                \"data_validation\": \"passed\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"‚úÖ Futures data saved: {len(processed_df)} records\")\n",
    "        else:\n",
    "            logger.warning(f\"Futures save failed: {save_result.error_message}\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Futures data processing error: {str(e)}\")\n",
    "        return None  # Graceful degradation\n",
    "\n",
    "# Fetch futures data with validation\n",
    "futures_df = fetch_futures_data(market_request)\n",
    "if futures_df is not None:\n",
    "    logger.info(f\"üìä Futures data shape: {futures_df.shape}\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Futures data not available, continuing without it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c62ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üîÑ Fetch Options Data\n",
    "# =====================================\n",
    "\n",
    "def fetch_options_data(request):\n",
    "    \"\"\"Fetch comprehensive options data with enhanced error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üîÑ Fetching options chain for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced option analyzer for comprehensive chain data\n",
    "        chain_result = option_analyzer.fetch_full_option_chain(\n",
    "            stock_code=request.stock_code,\n",
    "            expiry_date=request.expiry_date,\n",
    "            current_price=request.current_price,\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date,\n",
    "            strike_range=800  # Configurable range\n",
    "        )\n",
    "        \n",
    "        if not chain_result.success:\n",
    "            logger.warning(f\"Options chain fetch failed: {chain_result.error_message}\")\n",
    "            return None\n",
    "        \n",
    "        options_df = chain_result.data\n",
    "        \n",
    "        # Validate options data structure\n",
    "        logger.info(\"üîí Validating options data structure...\")\n",
    "        required_options_columns = ['open', 'high', 'low', 'close', 'volume', 'datetime', 'strike', 'right']\n",
    "        options_df = validate_dataframe_structure(options_df, required_options_columns, \"options data processing\")\n",
    "        \n",
    "        # Ensure datetime column is properly formatted\n",
    "        options_df = validate_datetime_column(options_df)\n",
    "        \n",
    "        # Ensure numeric columns are properly typed\n",
    "        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'strike']\n",
    "        options_df = ensure_numeric_columns(options_df, numeric_columns)\n",
    "        \n",
    "        # Validate options-specific data\n",
    "        def validate_options_specific():\n",
    "            # Validate option rights\n",
    "            if 'right' in options_df.columns:\n",
    "                valid_rights = {'call', 'put', 'c', 'p', 'CE', 'PE'}\n",
    "                invalid_rights = set(options_df['right'].dropna().astype(str).str.lower().unique()) - {r.lower() for r in valid_rights}\n",
    "                if invalid_rights:\n",
    "                    logger.warning(f\"Found invalid option rights: {invalid_rights}\")\n",
    "                    # Standardize option rights\n",
    "                    options_df['right'] = options_df['right'].astype(str).str.upper()\n",
    "                    options_df['right'] = options_df['right'].replace({'C': 'CE', 'P': 'PE', 'CALL': 'CE', 'PUT': 'PE'})\n",
    "            \n",
    "            # Validate strike prices\n",
    "            if 'strike' in options_df.columns:\n",
    "                invalid_strikes = options_df['strike'] <= 0\n",
    "                if invalid_strikes.any():\n",
    "                    logger.warning(f\"Found {invalid_strikes.sum()} invalid strike prices\")\n",
    "                    options_df.loc[invalid_strikes, 'strike'] = np.nan\n",
    "            \n",
    "            # Check for reasonable option premiums\n",
    "            if 'close' in options_df.columns and 'strike' in options_df.columns:\n",
    "                unreasonable_premiums = options_df['close'] > options_df['strike']\n",
    "                if unreasonable_premiums.any():\n",
    "                    logger.warning(f\"Found {unreasonable_premiums.sum()} options with premiums > strike prices\")\n",
    "        \n",
    "        safe_column_operation(options_df, validate_options_specific, \n",
    "                            ['right', 'strike', 'close'], \"options data validation\")\n",
    "        \n",
    "        # Check data sufficiency\n",
    "        if len(options_df) < 5:\n",
    "            logger.warning(f\"Very limited options data: only {len(options_df)} records\")\n",
    "            return None\n",
    "        \n",
    "        # Count unique strikes and rights\n",
    "        def get_options_summary():\n",
    "            unique_strikes = len(options_df['strike'].dropna().unique()) if 'strike' in options_df.columns else 0\n",
    "            unique_rights = len(options_df['right'].dropna().unique()) if 'right' in options_df.columns else 0\n",
    "            return unique_strikes, unique_rights\n",
    "        \n",
    "        unique_strikes, unique_rights = safe_column_operation(\n",
    "            options_df, get_options_summary, ['strike', 'right'], \"options summary\", (0, 0)\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"üìä Options data summary: {unique_strikes} strikes, {unique_rights} rights\")\n",
    "        \n",
    "        # Process options with specialized processor\n",
    "        logger.info(\"üìä Processing options with specialized processor...\")\n",
    "        processing_result = options_processor.process_options_dataframe(\n",
    "            options_df,\n",
    "            current_price=request.current_price,\n",
    "            add_greeks=True,\n",
    "            add_technical_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Options processing failed: {processing_result.error_message}\")\n",
    "            processed_df = options_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "        \n",
    "        # Final validation\n",
    "        if processed_df.empty:\n",
    "            logger.warning(\"Processed options data is empty\")\n",
    "            return None\n",
    "        \n",
    "        # Safe calculation of strike count for metadata\n",
    "        def safe_strike_count():\n",
    "            return len(processed_df['strike'].unique()) if 'strike' in processed_df.columns else 0\n",
    "        \n",
    "        strike_count = safe_column_operation(\n",
    "            processed_df, safe_strike_count, ['strike'], \"strike count calculation\", 0\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"‚úÖ Options data validated: {len(processed_df)} records, {len(processed_df.columns)} features\")\n",
    "        \n",
    "        # Save with comprehensive metadata\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_options_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"options\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"expiry_date\": request.expiry_date,\n",
    "                \"current_price\": request.current_price,\n",
    "                \"strike_count\": strike_count,\n",
    "                \"total_records\": len(processed_df),\n",
    "                \"data_validation\": \"passed\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"‚úÖ Options data saved: {len(processed_df)} records, {strike_count} strikes\")\n",
    "        else:\n",
    "            logger.warning(f\"Options save failed: {save_result.error_message}\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Options data processing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fetch options data with validation\n",
    "options_df = fetch_options_data(market_request)\n",
    "if options_df is not None:\n",
    "    logger.info(f\"üîÑ Options data shape: {options_df.shape}\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Options data not available, continuing without it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe69aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üîó Data Combination and Enhancement\n",
    "# =====================================\n",
    "\n",
    "from data_processing_utils import ProcessingResult, DataQuality\n",
    "\n",
    "def combine_and_enhance_data(equity_df, futures_df=None, options_df=None):\n",
    "    \"\"\"Combine and enhance all datasets with comprehensive error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(\"üîó Starting data combination and enhancement\")\n",
    "        \n",
    "        # Validate input datasets\n",
    "        if equity_df is None or equity_df.empty:\n",
    "            raise ValueError(\"Equity data is required and cannot be empty\")\n",
    "        \n",
    "        # Ensure all datasets have datetime column\n",
    "        datasets_to_validate = [(equity_df, \"equity\")]\n",
    "        if futures_df is not None and not futures_df.empty:\n",
    "            datasets_to_validate.append((futures_df, \"futures\"))\n",
    "        if options_df is not None and not options_df.empty:\n",
    "            datasets_to_validate.append((options_df, \"options\"))\n",
    "        \n",
    "        validated_datasets = {}\n",
    "        for df, name in datasets_to_validate:\n",
    "            logger.info(f\"üîí Validating {name} dataset for combination...\")\n",
    "            \n",
    "            # Ensure datetime column exists\n",
    "            df_validated = validate_datetime_column(df)\n",
    "            \n",
    "            # Ensure basic numeric columns exist\n",
    "            basic_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "            df_validated = validate_dataframe_structure(df_validated, basic_columns, f\"{name} data combination\")\n",
    "            df_validated = ensure_numeric_columns(df_validated, basic_columns)\n",
    "            \n",
    "            # Remove any completely empty rows\n",
    "            df_validated = df_validated.dropna(how='all')\n",
    "            \n",
    "            if df_validated.empty:\n",
    "                logger.warning(f\"‚ö†Ô∏è {name} dataset is empty after validation, excluding from combination\")\n",
    "                continue\n",
    "                \n",
    "            validated_datasets[name] = df_validated\n",
    "            logger.info(f\"‚úÖ {name} dataset validated: {df_validated.shape}\")\n",
    "        \n",
    "        if 'equity' not in validated_datasets:\n",
    "            raise ValueError(\"Equity data validation failed - cannot proceed with combination\")\n",
    "        \n",
    "        # Use enhanced options processor for data combination\n",
    "        logger.info(\"üîó Combining validated datasets...\")\n",
    "        combination_result = options_processor.combine_market_data(\n",
    "            equity_data=validated_datasets['equity'],\n",
    "            futures_data=validated_datasets.get('futures'),\n",
    "            options_data=validated_datasets.get('options')\n",
    "        )\n",
    "        \n",
    "        if not combination_result.success:\n",
    "            logger.warning(f\"Data combination had issues: {combination_result.error_message}\")\n",
    "            # Fallback to equity data only\n",
    "            combined_df = validated_datasets['equity'].copy()\n",
    "            logger.info(\"üìà Falling back to equity data only\")\n",
    "        else:\n",
    "            combined_df = combination_result.data\n",
    "            logger.info(f\"‚úÖ Data combined successfully: {combined_df.shape}\")\n",
    "        \n",
    "        # Validate combined dataset\n",
    "        logger.info(\"üîí Validating combined dataset...\")\n",
    "        if combined_df.empty:\n",
    "            raise ValueError(\"Combined dataset is empty\")\n",
    "        \n",
    "        # Ensure datetime column is properly set\n",
    "        combined_df = validate_datetime_column(combined_df)\n",
    "        \n",
    "        # Check for and handle infinite values\n",
    "        def clean_infinite_values():\n",
    "            numeric_cols = combined_df.select_dtypes(include=[np.number]).columns\n",
    "            inf_counts = {}\n",
    "            for col in numeric_cols:\n",
    "                inf_count = np.isinf(combined_df[col]).sum()\n",
    "                if inf_count > 0:\n",
    "                    inf_counts[col] = inf_count\n",
    "                    combined_df[col] = combined_df[col].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            if inf_counts:\n",
    "                logger.warning(f\"Replaced infinite values in columns: {inf_counts}\")\n",
    "            \n",
    "            return len(inf_counts)\n",
    "        \n",
    "        inf_replacements = safe_column_operation(\n",
    "            combined_df, clean_infinite_values, [], \"infinite value cleanup\", 0\n",
    "        )\n",
    "        \n",
    "        # Enhance with relationship metadata using options processor\n",
    "        logger.info(\"üîó Adding relationship features...\")\n",
    "        enhancement_result = options_processor.add_relationship_features(\n",
    "            combined_df,\n",
    "            include_correlations=True,\n",
    "            include_price_targets=True\n",
    "        )\n",
    "        \n",
    "        if not enhancement_result.success:\n",
    "            logger.warning(f\"Enhancement failed: {enhancement_result.error_message}\")\n",
    "            enhanced_df = combined_df\n",
    "        else:\n",
    "            enhanced_df = enhancement_result.data\n",
    "        \n",
    "        # Final validation of enhanced dataset\n",
    "        logger.info(\"üîí Final validation of enhanced dataset...\")\n",
    "        \n",
    "        # Check data quality\n",
    "        def assess_final_quality():\n",
    "            total_cells = enhanced_df.shape[0] * enhanced_df.shape[1]\n",
    "            nan_cells = enhanced_df.isna().sum().sum()\n",
    "            nan_percentage = (nan_cells / total_cells) * 100 if total_cells > 0 else 100\n",
    "            \n",
    "            logger.info(f\"üìä Data quality: {nan_percentage:.1f}% NaN values\")\n",
    "            \n",
    "            if nan_percentage < 10:\n",
    "                return \"excellent\"\n",
    "            elif nan_percentage < 25:\n",
    "                return \"good\"\n",
    "            elif nan_percentage < 50:\n",
    "                return \"fair\"\n",
    "            else:\n",
    "                return \"poor\"\n",
    "        \n",
    "        data_quality = safe_column_operation(\n",
    "            enhanced_df, assess_final_quality, [], \"data quality assessment\", \"unknown\"\n",
    "        )\n",
    "        \n",
    "        # Data quality assessment using options processor\n",
    "        quality_result = options_processor.assess_data_quality(enhanced_df)\n",
    "        quality_score = quality_result.metadata.get('quality_score', 'N/A') if quality_result.success else 'N/A'\n",
    "        \n",
    "        logger.info(f\"üìä Data quality assessment: {quality_score}\")\n",
    "        \n",
    "        # Safe calculation of feature counts\n",
    "        def calculate_feature_breakdown():\n",
    "            equity_features = len([c for c in enhanced_df.columns if c.startswith('equity_')])\n",
    "            futures_features = len([c for c in enhanced_df.columns if c.startswith('futures_')])\n",
    "            options_features = len([c for c in enhanced_df.columns if c.startswith('options_')])\n",
    "            relationship_features = len([c for c in enhanced_df.columns if any(keyword in c for keyword in \n",
    "                                      ['corr_', 'basis_', 'divergence', 'ratio'])])\n",
    "            return equity_features, futures_features, options_features, relationship_features\n",
    "        \n",
    "        equity_feat, futures_feat, options_feat, relationship_feat = safe_column_operation(\n",
    "            enhanced_df, calculate_feature_breakdown, [], \"feature breakdown\", (0, 0, 0, 0)\n",
    "        )\n",
    "        \n",
    "        # Save final enhanced dataset\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            enhanced_df,\n",
    "            \"tcs_enhanced_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"combined_enhanced\",\n",
    "                \"features_count\": len(enhanced_df.columns),\n",
    "                \"records_count\": len(enhanced_df),\n",
    "                \"data_quality\": quality_score,\n",
    "                \"data_validation\": \"comprehensive_passed\",\n",
    "                \"equity_features\": equity_feat,\n",
    "                \"futures_features\": futures_feat,\n",
    "                \"options_features\": options_feat,\n",
    "                \"relationship_features\": relationship_feat,\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"‚úÖ Enhanced dataset saved: {enhanced_df.shape} with {len(enhanced_df.columns)} features\")\n",
    "        else:\n",
    "            logger.warning(f\"Enhanced data save failed: {save_result.error_message}\")\n",
    "        \n",
    "        return enhanced_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data combination and enhancement failed: {str(e)}\")\n",
    "        # Provide graceful fallback with basic validation\n",
    "        if equity_df is not None and not equity_df.empty:\n",
    "            logger.info(\"üìà Falling back to validated equity data only\")\n",
    "            fallback_df = validate_dataframe_structure(equity_df, ['open', 'high', 'low', 'close'], \"fallback\")\n",
    "            fallback_df = validate_datetime_column(fallback_df)\n",
    "            return fallback_df\n",
    "        else:\n",
    "            raise ValueError(\"Cannot create fallback dataset - equity data is invalid\")\n",
    "\n",
    "# Combine and enhance all data with comprehensive validation\n",
    "try:\n",
    "    enhanced_df = combine_and_enhance_data(equity_df, futures_df, options_df)\n",
    "    \n",
    "    # Final data summary with safe operations\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"üìä FINAL DATA SUMMARY\")\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(f\"üìà Total records: {len(enhanced_df):,}\")\n",
    "    logger.info(f\"üìä Total features: {len(enhanced_df.columns):,}\")\n",
    "    \n",
    "    # Safe date range calculation\n",
    "    def get_date_range():\n",
    "        if 'datetime' in enhanced_df.columns and not enhanced_df['datetime'].isna().all():\n",
    "            return enhanced_df['datetime'].min(), enhanced_df['datetime'].max()\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    \n",
    "    min_date, max_date = safe_column_operation(\n",
    "        enhanced_df, get_date_range, ['datetime'], \"date range calculation\", (\"Unknown\", \"Unknown\")\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"üìÖ Date range: {min_date} to {max_date}\")\n",
    "    \n",
    "    # Feature breakdown with safe calculations\n",
    "    def safe_feature_count(prefix):\n",
    "        return len([c for c in enhanced_df.columns if c.startswith(prefix)])\n",
    "    \n",
    "    equity_features = safe_feature_count('equity_')\n",
    "    futures_features = safe_feature_count('futures_')\n",
    "    options_features = safe_feature_count('options_')\n",
    "    relationship_features = len([c for c in enhanced_df.columns if any(keyword in c for keyword in \n",
    "                                ['corr_', 'basis_', 'divergence', 'ratio'])])\n",
    "    \n",
    "    logger.info(f\"üìà Equity features: {equity_features}\")\n",
    "    logger.info(f\"üìä Futures features: {futures_features}\")\n",
    "    logger.info(f\"üîÑ Options features: {options_features}\")\n",
    "    logger.info(f\"üîó Relationship features: {relationship_features}\")\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"‚úÖ‚úÖ‚úÖ ALL DATA PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    logger.info(\"üîí Data validation and type safety measures applied\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Critical error in data processing: {str(e)}\")\n",
    "    # Provide graceful fallback\n",
    "    if 'equity_df' in locals() and equity_df is not None and not equity_df.empty:\n",
    "        logger.info(\"üìà Falling back to equity data only with validation\")\n",
    "        enhanced_df = validate_dataframe_structure(equity_df, ['open', 'high', 'low', 'close'], \"final fallback\")\n",
    "        enhanced_df = validate_datetime_column(enhanced_df)\n",
    "        logger.info(f\"üìà Fallback dataset shape: {enhanced_df.shape}\")\n",
    "    else:\n",
    "        logger.error(\"üí• Complete failure - no valid data available\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
