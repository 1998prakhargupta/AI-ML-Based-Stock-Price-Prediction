{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b96bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 🛠 Enhanced Modular Data Collection\n",
    "# =====================================\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Import our enhanced utilities\n",
    "from app_config import Config\n",
    "from enhanced_breeze_utils import EnhancedBreezeDataManager, OptionChainAnalyzer\n",
    "from data_processing_utils import TechnicalIndicatorProcessor, OptionsDataProcessor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =====================================\n",
    "# 🛠 Initialize Enhanced Data Manager\n",
    "# =====================================\n",
    "\n",
    "try:\n",
    "    # Initialize configuration and enhanced data manager\n",
    "    config = Config()\n",
    "    data_manager = EnhancedBreezeDataManager()\n",
    "    \n",
    "    # Initialize processing utilities\n",
    "    indicator_processor = TechnicalIndicatorProcessor()\n",
    "    options_processor = OptionsDataProcessor()\n",
    "    option_analyzer = OptionChainAnalyzer()\n",
    "    \n",
    "    # Set up Google Drive if in Colab environment\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        logger.info(\"✅ Google Drive mounted\")\n",
    "    except ImportError:\n",
    "        logger.info(\"ℹ️ Not in Colab environment, skipping Google Drive mount\")\n",
    "    \n",
    "    # Authenticate with enhanced retry logic\n",
    "    auth_result = data_manager.authenticate()\n",
    "    if auth_result.success:\n",
    "        logger.info(\"✅ Breeze API authenticated successfully\")\n",
    "        breeze = data_manager.breeze\n",
    "    else:\n",
    "        logger.error(f\"❌ Authentication failed: {auth_result.error_message}\")\n",
    "        raise Exception(f\"Authentication failed: {auth_result.error_message}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Critical initialization error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"✅ All modules and enhanced utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a66366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 📊 Data Validation Framework\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Optional, Union, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "def validate_dataframe_structure(df: pd.DataFrame, required_columns: List[str], \n",
    "                               optional_columns: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate DataFrame has required columns and add missing ones with NaN.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        required_columns: List of required column names\n",
    "        optional_columns: List of optional column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all required columns (missing ones filled with NaN)\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logging.warning(\"Empty or None DataFrame provided\")\n",
    "        # Create empty DataFrame with required columns\n",
    "        return pd.DataFrame(columns=required_columns)\n",
    "    \n",
    "    missing_required = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_required:\n",
    "        logging.warning(f\"Missing required columns: {missing_required}\")\n",
    "        for col in missing_required:\n",
    "            df[col] = np.nan\n",
    "    \n",
    "    # Log available optional columns\n",
    "    if optional_columns:\n",
    "        available_optional = [col for col in optional_columns if col in df.columns]\n",
    "        logging.info(f\"Available optional columns: {available_optional}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensure_numeric_columns(df: pd.DataFrame, columns: List[str], \n",
    "                         fill_method: str = 'forward') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure specified columns are numeric with proper type conversion and error handling.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        columns: List of column names to convert to numeric\n",
    "        fill_method: Method to fill NaN values ('forward', 'backward', 'zero', 'mean')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with numeric columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            try:\n",
    "                # Convert to numeric, coercing errors to NaN\n",
    "                df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "                \n",
    "                # Handle NaN values based on fill_method\n",
    "                if fill_method == 'forward':\n",
    "                    df_copy[col] = df_copy[col].fillna(method='ffill')\n",
    "                elif fill_method == 'backward':\n",
    "                    df_copy[col] = df_copy[col].fillna(method='bfill')\n",
    "                elif fill_method == 'zero':\n",
    "                    df_copy[col] = df_copy[col].fillna(0)\n",
    "                elif fill_method == 'mean':\n",
    "                    df_copy[col] = df_copy[col].fillna(df_copy[col].mean())\n",
    "                \n",
    "                logging.info(f\"Successfully converted {col} to numeric\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to convert {col} to numeric: {str(e)}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def validate_datetime_column(df: pd.DataFrame, datetime_col: str = 'datetime') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate and format datetime column with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        datetime_col: Name of datetime column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with properly formatted datetime column\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if datetime_col not in df_copy.columns:\n",
    "        # Try common datetime column names\n",
    "        datetime_alternatives = ['date', 'timestamp', 'time', 'Date', 'DateTime']\n",
    "        found_col = None\n",
    "        for alt in datetime_alternatives:\n",
    "            if alt in df_copy.columns:\n",
    "                found_col = alt\n",
    "                break\n",
    "        \n",
    "        if found_col:\n",
    "            logging.info(f\"Using {found_col} as datetime column\")\n",
    "            df_copy[datetime_col] = df_copy[found_col]\n",
    "        else:\n",
    "            logging.warning(f\"No datetime column found, creating default index-based datetime\")\n",
    "            df_copy[datetime_col] = pd.date_range(start='2023-01-01', periods=len(df_copy), freq='D')\n",
    "            return df_copy\n",
    "    \n",
    "    try:\n",
    "        # Convert to datetime with error handling\n",
    "        df_copy[datetime_col] = pd.to_datetime(df_copy[datetime_col], errors='coerce')\n",
    "        \n",
    "        # Remove rows with invalid datetime\n",
    "        invalid_datetime_count = df_copy[datetime_col].isna().sum()\n",
    "        if invalid_datetime_count > 0:\n",
    "            logging.warning(f\"Removing {invalid_datetime_count} rows with invalid datetime\")\n",
    "            df_copy = df_copy.dropna(subset=[datetime_col])\n",
    "        \n",
    "        # Sort by datetime\n",
    "        df_copy = df_copy.sort_values(datetime_col).reset_index(drop=True)\n",
    "        \n",
    "        logging.info(f\"Successfully validated datetime column: {len(df_copy)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"DateTime validation failed: {str(e)}\")\n",
    "        # Fallback: create sequential datetime\n",
    "        df_copy[datetime_col] = pd.date_range(start='2023-01-01', periods=len(df_copy), freq='D')\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def safe_column_operation(df: pd.DataFrame, operation_func, *args, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safely perform operations that depend on specific columns with graceful error handling.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        operation_func: Function to perform on DataFrame\n",
    "        *args, **kwargs: Arguments for the operation function\n",
    "    \n",
    "    Returns:\n",
    "        Result of operation or original DataFrame if operation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = operation_func(df, *args, **kwargs)\n",
    "        logging.info(f\"Successfully performed operation: {operation_func.__name__}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Operation {operation_func.__name__} failed: {str(e)}, returning original DataFrame\")\n",
    "        return df\n",
    "\n",
    "logging.info(\"✅ Data validation utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 🛠 Parameter Setup with Validation\n",
    "# =====================================\n",
    "\n",
    "from enhanced_breeze_utils import MarketDataRequest\n",
    "from data_processing_utils import ValidationError\n",
    "\n",
    "def setup_trading_parameters():\n",
    "    \"\"\"Setup and validate trading parameters with proper error handling\"\"\"\n",
    "    try:\n",
    "        # Basic parameters\n",
    "        stock_name = \"TCS\"\n",
    "        interval = \"5minute\"\n",
    "        \n",
    "        # Get trading dates using enhanced utilities\n",
    "        date_result = data_manager.get_trading_dates(days_back=30)\n",
    "        if not date_result.success:\n",
    "            raise ValidationError(f\"Failed to get trading dates: {date_result.error_message}\")\n",
    "        \n",
    "        from_date = date_result.data['from_date']\n",
    "        to_date = date_result.data['to_date']\n",
    "        \n",
    "        logger.info(f\"📅 Trading period: {from_date} to {to_date}\")\n",
    "        \n",
    "        # Get current LTP with enhanced error handling\n",
    "        ltp_result = data_manager.get_live_price(stock_name, \"NSE\")\n",
    "        if not ltp_result.success:\n",
    "            raise ValidationError(f\"Failed to get LTP: {ltp_result.error_message}\")\n",
    "        \n",
    "        ltp = ltp_result.data['ltp']\n",
    "        logger.info(f\"📦 Current LTP for {stock_name}: {ltp}\")\n",
    "        \n",
    "        # Get valid expiry using enhanced option analyzer\n",
    "        expiry_result = option_analyzer.get_next_valid_expiry(stock_name)\n",
    "        if not expiry_result.success:\n",
    "            raise ValidationError(f\"Failed to get expiry: {expiry_result.error_message}\")\n",
    "        \n",
    "        expiry_date = expiry_result.data['expiry_date']\n",
    "        logger.info(f\"📌 Using expiry: {expiry_date}\")\n",
    "        \n",
    "        # Create structured request object\n",
    "        request = MarketDataRequest(\n",
    "            stock_code=stock_name,\n",
    "            exchange_code=\"NSE\",\n",
    "            interval=interval,\n",
    "            from_date=from_date,\n",
    "            to_date=to_date,\n",
    "            expiry_date=expiry_date,\n",
    "            current_price=ltp\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Parameter setup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Setup parameters\n",
    "market_request = setup_trading_parameters()\n",
    "logger.info(\"✅ Parameters setup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 📈 Fetch Equity Data with Validation\n",
    "# =====================================\n",
    "\n",
    "def fetch_equity_data(request):\n",
    "    \"\"\"Fetch equity data with comprehensive validation and error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"📊 Fetching equity data for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced data manager for equity data\n",
    "        equity_result = data_manager.fetch_historical_data(\n",
    "            stock_code=request.stock_code,\n",
    "            exchange_code=request.exchange_code,\n",
    "            product_type=\"cash\",\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date\n",
    "        )\n",
    "        \n",
    "        if not equity_result.success:\n",
    "            raise ValidationError(f\"Equity data fetch failed: {equity_result.error_message}\")\n",
    "        \n",
    "        equity_df = equity_result.data\n",
    "        \n",
    "        # ===== DATA VALIDATION =====\n",
    "        logger.info(\"🔍 Starting equity data validation...\")\n",
    "        \n",
    "        # 1. Validate required OHLCV columns\n",
    "        required_equity_cols = ['open', 'high', 'low', 'close', 'volume', 'datetime']\n",
    "        equity_df = validate_dataframe_structure(equity_df, required_equity_cols)\n",
    "        \n",
    "        # 2. Validate datetime column\n",
    "        equity_df = validate_datetime_column(equity_df, 'datetime')\n",
    "        \n",
    "        # 3. Ensure numeric columns are properly typed\n",
    "        numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        equity_df = ensure_numeric_columns(equity_df, numeric_cols, fill_method='forward')\n",
    "        \n",
    "        # 4. OHLC logic validation and fixing\n",
    "        def validate_ohlc_logic(df):\n",
    "            \"\"\"Validate and fix OHLC logic inconsistencies\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            issues_fixed = 0\n",
    "            \n",
    "            for idx in df_copy.index:\n",
    "                try:\n",
    "                    o, h, l, c = df_copy.loc[idx, ['open', 'high', 'low', 'close']]\n",
    "                    \n",
    "                    if pd.isna([o, h, l, c]).any():\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if high is actually the highest\n",
    "                    actual_high = max(o, h, l, c)\n",
    "                    if h < actual_high:\n",
    "                        df_copy.loc[idx, 'high'] = actual_high\n",
    "                        issues_fixed += 1\n",
    "                    \n",
    "                    # Check if low is actually the lowest\n",
    "                    actual_low = min(o, h, l, c)\n",
    "                    if l > actual_low:\n",
    "                        df_copy.loc[idx, 'low'] = actual_low\n",
    "                        issues_fixed += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"OHLC validation failed for index {idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if issues_fixed > 0:\n",
    "                logger.info(f\"Fixed {issues_fixed} OHLC logic inconsistencies\")\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        equity_df = safe_column_operation(equity_df, validate_ohlc_logic)\n",
    "        \n",
    "        # 5. Remove infinite values and extreme outliers\n",
    "        def clean_extreme_values(df):\n",
    "            \"\"\"Clean infinite values and extreme outliers\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Replace infinite values\n",
    "            df_copy = df_copy.replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            # Remove extreme outliers (beyond 10 standard deviations)\n",
    "            for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "                if col in df_copy.columns:\n",
    "                    mean_val = df_copy[col].mean()\n",
    "                    std_val = df_copy[col].std()\n",
    "                    \n",
    "                    if not pd.isna(std_val) and std_val > 0:\n",
    "                        outlier_mask = np.abs(df_copy[col] - mean_val) > (10 * std_val)\n",
    "                        outlier_count = outlier_mask.sum()\n",
    "                        \n",
    "                        if outlier_count > 0:\n",
    "                            logger.warning(f\"Removing {outlier_count} extreme outliers from {col}\")\n",
    "                            df_copy.loc[outlier_mask, col] = np.nan\n",
    "            \n",
    "            # Forward fill any new NaN values\n",
    "            df_copy = df_copy.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        equity_df = safe_column_operation(equity_df, clean_extreme_values)\n",
    "        \n",
    "        # ===== TECHNICAL INDICATORS PROCESSING =====\n",
    "        # Process with technical indicators using enhanced processor\n",
    "        processing_result = indicator_processor.process_dataframe(\n",
    "            equity_df,\n",
    "            add_all_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Technical indicator processing had issues: {processing_result.error_message}\")\n",
    "            # Continue with validated raw data if indicator processing fails\n",
    "            processed_df = equity_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "            \n",
    "            # Validate technical indicators\n",
    "            def validate_technical_indicators(df):\n",
    "                \"\"\"Validate technical indicators for reasonableness\"\"\"\n",
    "                df_copy = df.copy()\n",
    "                \n",
    "                # Check RSI values (should be between 0 and 100)\n",
    "                rsi_cols = [col for col in df_copy.columns if 'rsi' in col.lower()]\n",
    "                for col in rsi_cols:\n",
    "                    invalid_rsi = (df_copy[col] < 0) | (df_copy[col] > 100)\n",
    "                    if invalid_rsi.any():\n",
    "                        logger.warning(f\"Found {invalid_rsi.sum()} invalid RSI values in {col}\")\n",
    "                        df_copy.loc[invalid_rsi, col] = np.nan\n",
    "                \n",
    "                # Check for extreme technical indicator values\n",
    "                tech_cols = [col for col in df_copy.columns \n",
    "                           if any(tech in col.lower() for tech in ['sma', 'ema', 'macd', 'bb', 'atr'])]\n",
    "                \n",
    "                for col in tech_cols:\n",
    "                    if df_copy[col].std() > 0:\n",
    "                        z_scores = np.abs((df_copy[col] - df_copy[col].mean()) / df_copy[col].std())\n",
    "                        extreme_mask = z_scores > 5\n",
    "                        \n",
    "                        if extreme_mask.any():\n",
    "                            logger.warning(f\"Found {extreme_mask.sum()} extreme values in {col}\")\n",
    "                            df_copy.loc[extreme_mask, col] = np.nan\n",
    "                \n",
    "                # Forward fill any new NaN values\n",
    "                df_copy = df_copy.fillna(method='ffill').fillna(method='bfill')\n",
    "                \n",
    "                return df_copy\n",
    "            \n",
    "            processed_df = safe_column_operation(processed_df, validate_technical_indicators)\n",
    "        \n",
    "        # ===== METADATA AND QUALITY ASSESSMENT =====\n",
    "        # Calculate data quality metrics\n",
    "        total_rows = len(processed_df)\n",
    "        non_null_rows = processed_df.dropna().shape[0]\n",
    "        data_quality_score = (non_null_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        logger.info(f\"📊 Data Quality Assessment:\")\n",
    "        logger.info(f\"  Total rows: {total_rows:,}\")\n",
    "        logger.info(f\"  Complete rows: {non_null_rows:,}\")\n",
    "        logger.info(f\"  Quality score: {data_quality_score:.2f}%\")\n",
    "        \n",
    "        # Save with comprehensive metadata\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_equity_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"equity\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"interval\": request.interval,\n",
    "                \"validation_timestamp\": datetime.now().isoformat(),\n",
    "                \"data_quality_score\": data_quality_score,\n",
    "                \"total_records\": total_rows,\n",
    "                \"complete_records\": non_null_rows,\n",
    "                \"indicators_count\": len([c for c in processed_df.columns if c not in ['datetime', 'open', 'high', 'low', 'close', 'volume']]),\n",
    "                \"validation_checks_passed\": [\n",
    "                    \"column_structure\", \"datetime_format\", \"numeric_types\", \n",
    "                    \"ohlc_logic\", \"outlier_removal\", \"technical_indicators\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"✅ Equity data saved: {len(processed_df)} records with {len(processed_df.columns)} features\")\n",
    "        else:\n",
    "            logger.warning(f\"Save failed: {save_result.error_message}\")\n",
    "        \n",
    "        logger.info(f\"✅ Equity data validation completed successfully\")\n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Equity data fetch failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Fetch equity data with validation\n",
    "equity_df = fetch_equity_data(market_request)\n",
    "logger.info(f\"📈 Equity data shape: {equity_df.shape}\")\n",
    "logger.info(f\"📈 Equity data columns: {list(equity_df.columns[:10])}{'...' if len(equity_df.columns) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f923f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 📈 Fetch Futures Data with Validation\n",
    "# =====================================\n",
    "\n",
    "def fetch_futures_data(request):\n",
    "    \"\"\"Fetch futures data with comprehensive validation and graceful degradation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"📊 Fetching futures data for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced data manager for futures data\n",
    "        futures_result = data_manager.fetch_historical_data(\n",
    "            stock_code=request.stock_code,\n",
    "            exchange_code=\"NFO\",\n",
    "            product_type=\"futures\",\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date,\n",
    "            expiry_date=request.expiry_date\n",
    "        )\n",
    "        \n",
    "        if not futures_result.success:\n",
    "            logger.warning(f\"Futures data fetch failed: {futures_result.error_message}\")\n",
    "            return None  # Return None instead of failing completely\n",
    "        \n",
    "        futures_df = futures_result.data\n",
    "        \n",
    "        # ===== DATA VALIDATION =====\n",
    "        logger.info(\"🔍 Starting futures data validation...\")\n",
    "        \n",
    "        # 1. Validate required columns for futures\n",
    "        required_futures_cols = ['open', 'high', 'low', 'close', 'volume', 'datetime']\n",
    "        optional_futures_cols = ['open_interest', 'oi', 'basis', 'premium']\n",
    "        \n",
    "        futures_df = validate_dataframe_structure(futures_df, required_futures_cols, optional_futures_cols)\n",
    "        \n",
    "        # 2. Validate datetime column\n",
    "        futures_df = validate_datetime_column(futures_df, 'datetime')\n",
    "        \n",
    "        # 3. Ensure numeric columns are properly typed\n",
    "        numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        if 'open_interest' in futures_df.columns:\n",
    "            numeric_cols.append('open_interest')\n",
    "        if 'oi' in futures_df.columns:\n",
    "            numeric_cols.append('oi')\n",
    "            \n",
    "        futures_df = ensure_numeric_columns(futures_df, numeric_cols, fill_method='forward')\n",
    "        \n",
    "        # 4. Futures-specific validation\n",
    "        def validate_futures_specifics(df):\n",
    "            \"\"\"Validate futures-specific requirements\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            validation_issues = []\n",
    "            \n",
    "            # Volume validation (futures should have meaningful volume)\n",
    "            if 'volume' in df_copy.columns:\n",
    "                zero_volume_count = (df_copy['volume'] <= 0).sum()\n",
    "                if zero_volume_count > len(df_copy) * 0.5:  # More than 50% zero volume\n",
    "                    validation_issues.append(f\"High zero volume: {zero_volume_count}/{len(df_copy)} records\")\n",
    "                \n",
    "                # Replace zero volumes with small positive values\n",
    "                df_copy.loc[df_copy['volume'] <= 0, 'volume'] = 1\n",
    "            \n",
    "            # Open Interest validation\n",
    "            oi_col = None\n",
    "            if 'open_interest' in df_copy.columns:\n",
    "                oi_col = 'open_interest'\n",
    "            elif 'oi' in df_copy.columns:\n",
    "                oi_col = 'oi'\n",
    "            \n",
    "            if oi_col:\n",
    "                # Open interest should generally be non-negative\n",
    "                negative_oi = (df_copy[oi_col] < 0).sum()\n",
    "                if negative_oi > 0:\n",
    "                    validation_issues.append(f\"Found {negative_oi} negative open interest values\")\n",
    "                    df_copy.loc[df_copy[oi_col] < 0, oi_col] = 0\n",
    "            \n",
    "            # Price continuity check\n",
    "            def check_price_continuity(prices, threshold=0.1):\n",
    "                \"\"\"Check for unrealistic price jumps\"\"\"\n",
    "                if len(prices) < 2:\n",
    "                    return []\n",
    "                \n",
    "                price_changes = prices.pct_change().abs()\n",
    "                extreme_changes = price_changes > threshold\n",
    "                return extreme_changes\n",
    "            \n",
    "            extreme_close_changes = check_price_continuity(df_copy['close'])\n",
    "            if extreme_close_changes.any():\n",
    "                extreme_count = extreme_close_changes.sum()\n",
    "                validation_issues.append(f\"Found {extreme_count} extreme price jumps (>10%)\")\n",
    "                # Log but don't automatically fix - might be legitimate gaps\n",
    "            \n",
    "            if validation_issues:\n",
    "                logger.warning(f\"Futures validation issues: {validation_issues}\")\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        futures_df = safe_column_operation(futures_df, validate_futures_specifics)\n",
    "        \n",
    "        # 5. Clean extreme values specific to futures\n",
    "        def clean_futures_extremes(df):\n",
    "            \"\"\"Clean futures-specific extreme values\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Replace infinite values\n",
    "            df_copy = df_copy.replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            # Volume outlier handling (different from equity)\n",
    "            if 'volume' in df_copy.columns and len(df_copy) > 10:\n",
    "                volume_median = df_copy['volume'].median()\n",
    "                volume_mad = (df_copy['volume'] - volume_median).abs().median()\n",
    "                \n",
    "                if volume_mad > 0:\n",
    "                    # Use median absolute deviation for volume outliers\n",
    "                    volume_outliers = np.abs(df_copy['volume'] - volume_median) > (10 * volume_mad)\n",
    "                    outlier_count = volume_outliers.sum()\n",
    "                    \n",
    "                    if outlier_count > 0:\n",
    "                        logger.warning(f\"Capping {outlier_count} volume outliers\")\n",
    "                        # Cap instead of removing for futures\n",
    "                        upper_cap = volume_median + (5 * volume_mad)\n",
    "                        df_copy.loc[volume_outliers, 'volume'] = upper_cap\n",
    "            \n",
    "            # Price outlier handling\n",
    "            for col in ['open', 'high', 'low', 'close']:\n",
    "                if col in df_copy.columns and len(df_copy) > 5:\n",
    "                    # Use rolling median for price outlier detection\n",
    "                    rolling_median = df_copy[col].rolling(window=5, center=True).median()\n",
    "                    rolling_mad = (df_copy[col] - rolling_median).abs().rolling(window=5, center=True).median()\n",
    "                    \n",
    "                    outlier_mask = np.abs(df_copy[col] - rolling_median) > (8 * rolling_mad)\n",
    "                    outlier_count = outlier_mask.sum()\n",
    "                    \n",
    "                    if outlier_count > 0:\n",
    "                        logger.warning(f\"Found {outlier_count} price outliers in {col}\")\n",
    "                        # For futures, cap rather than remove outliers\n",
    "                        df_copy.loc[outlier_mask, col] = rolling_median[outlier_mask]\n",
    "            \n",
    "            # Fill any remaining NaN values\n",
    "            df_copy = df_copy.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        futures_df = safe_column_operation(futures_df, clean_futures_extremes)\n",
    "        \n",
    "        # ===== TECHNICAL INDICATORS PROCESSING =====\n",
    "        # Process with technical indicators\n",
    "        processing_result = indicator_processor.process_dataframe(\n",
    "            futures_df,\n",
    "            add_all_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Futures technical indicator processing failed: {processing_result.error_message}\")\n",
    "            processed_df = futures_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "        \n",
    "        # ===== DATA QUALITY ASSESSMENT =====\n",
    "        total_rows = len(processed_df)\n",
    "        non_null_rows = processed_df.dropna().shape[0]\n",
    "        data_quality_score = (non_null_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        # Futures-specific quality checks\n",
    "        volume_quality = \"Good\" if processed_df['volume'].mean() > 100 else \"Low\"\n",
    "        \n",
    "        oi_quality = \"N/A\"\n",
    "        if 'open_interest' in processed_df.columns:\n",
    "            oi_mean = processed_df['open_interest'].mean()\n",
    "            oi_quality = \"Good\" if oi_mean > 1000 else \"Low\"\n",
    "        elif 'oi' in processed_df.columns:\n",
    "            oi_mean = processed_df['oi'].mean()\n",
    "            oi_quality = \"Good\" if oi_mean > 1000 else \"Low\"\n",
    "        \n",
    "        logger.info(f\"📊 Futures Data Quality Assessment:\")\n",
    "        logger.info(f\"  Total rows: {total_rows:,}\")\n",
    "        logger.info(f\"  Complete rows: {non_null_rows:,}\")\n",
    "        logger.info(f\"  Quality score: {data_quality_score:.2f}%\")\n",
    "        logger.info(f\"  Volume quality: {volume_quality}\")\n",
    "        logger.info(f\"  Open Interest quality: {oi_quality}\")\n",
    "        \n",
    "        # Save with comprehensive metadata\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_futures_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"futures\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"expiry_date\": request.expiry_date,\n",
    "                \"interval\": request.interval,\n",
    "                \"validation_timestamp\": datetime.now().isoformat(),\n",
    "                \"data_quality_score\": data_quality_score,\n",
    "                \"volume_quality\": volume_quality,\n",
    "                \"oi_quality\": oi_quality,\n",
    "                \"total_records\": total_rows,\n",
    "                \"complete_records\": non_null_rows,\n",
    "                \"validation_checks_passed\": [\n",
    "                    \"column_structure\", \"datetime_format\", \"numeric_types\",\n",
    "                    \"futures_specifics\", \"volume_validation\", \"price_continuity\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"✅ Futures data saved: {len(processed_df)} records\")\n",
    "        else:\n",
    "            logger.warning(f\"Futures save failed: {save_result.error_message}\")\n",
    "        \n",
    "        logger.info(f\"✅ Futures data validation completed successfully\")\n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Futures data processing error: {str(e)}\")\n",
    "        return None  # Graceful degradation\n",
    "\n",
    "# Fetch futures data with validation\n",
    "futures_df = fetch_futures_data(market_request)\n",
    "if futures_df is not None:\n",
    "    logger.info(f\"📈 Futures data shape: {futures_df.shape}\")\n",
    "    logger.info(f\"📈 Futures data quality validated successfully\")\n",
    "else:\n",
    "    logger.warning(\"⚠️ Futures data not available, continuing without it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c62ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 🔄 Fetch Options Data with Validation\n",
    "# =====================================\n",
    "\n",
    "def fetch_options_data(request):\n",
    "    \"\"\"Fetch comprehensive options data with enhanced validation and error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"🔄 Fetching options chain for {request.stock_code}\")\n",
    "        \n",
    "        # Use enhanced option analyzer for comprehensive chain data\n",
    "        chain_result = option_analyzer.fetch_full_option_chain(\n",
    "            stock_code=request.stock_code,\n",
    "            expiry_date=request.expiry_date,\n",
    "            current_price=request.current_price,\n",
    "            interval=request.interval,\n",
    "            from_date=request.from_date,\n",
    "            to_date=request.to_date,\n",
    "            strike_range=800  # Configurable range\n",
    "        )\n",
    "        \n",
    "        if not chain_result.success:\n",
    "            logger.warning(f\"Options chain fetch failed: {chain_result.error_message}\")\n",
    "            return None\n",
    "        \n",
    "        options_df = chain_result.data\n",
    "        \n",
    "        # ===== DATA VALIDATION =====\n",
    "        logger.info(\"🔍 Starting options data validation...\")\n",
    "        \n",
    "        # 1. Validate required columns for options\n",
    "        required_options_cols = ['strike', 'option_type', 'premium', 'datetime']\n",
    "        optional_options_cols = [\n",
    "            'open', 'high', 'low', 'close', 'volume', 'open_interest', 'oi',\n",
    "            'iv', 'implied_volatility', 'delta', 'gamma', 'theta', 'vega', 'rho'\n",
    "        ]\n",
    "        \n",
    "        options_df = validate_dataframe_structure(options_df, required_options_cols, optional_options_cols)\n",
    "        \n",
    "        # 2. Validate datetime column\n",
    "        options_df = validate_datetime_column(options_df, 'datetime')\n",
    "        \n",
    "        # 3. Ensure numeric columns are properly typed\n",
    "        numeric_cols = ['strike', 'premium']\n",
    "        price_cols = ['open', 'high', 'low', 'close']\n",
    "        vol_cols = ['volume', 'open_interest', 'oi']\n",
    "        greek_cols = ['iv', 'implied_volatility', 'delta', 'gamma', 'theta', 'vega', 'rho']\n",
    "        \n",
    "        # Add existing columns to numeric list\n",
    "        for col_group in [price_cols, vol_cols, greek_cols]:\n",
    "            numeric_cols.extend([col for col in col_group if col in options_df.columns])\n",
    "        \n",
    "        options_df = ensure_numeric_columns(options_df, numeric_cols, fill_method='forward')\n",
    "        \n",
    "        # 4. Options-specific validation\n",
    "        def validate_options_specifics(df):\n",
    "            \"\"\"Validate options-specific requirements\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            validation_issues = []\n",
    "            \n",
    "            # Strike price validation\n",
    "            if 'strike' in df_copy.columns:\n",
    "                invalid_strikes = (df_copy['strike'] <= 0).sum()\n",
    "                if invalid_strikes > 0:\n",
    "                    validation_issues.append(f\"Found {invalid_strikes} invalid strike prices\")\n",
    "                    df_copy = df_copy[df_copy['strike'] > 0]\n",
    "                \n",
    "                # Check strike price reasonableness relative to current price\n",
    "                current_price = request.current_price\n",
    "                if current_price > 0:\n",
    "                    strike_range_check = (\n",
    "                        (df_copy['strike'] > current_price * 2.5) | \n",
    "                        (df_copy['strike'] < current_price * 0.4)\n",
    "                    )\n",
    "                    extreme_strikes = strike_range_check.sum()\n",
    "                    if extreme_strikes > 0:\n",
    "                        validation_issues.append(f\"Found {extreme_strikes} strikes far from current price\")\n",
    "            \n",
    "            # Option type validation\n",
    "            if 'option_type' in df_copy.columns:\n",
    "                # Standardize option type formats\n",
    "                df_copy['option_type'] = df_copy['option_type'].astype(str).str.upper()\n",
    "                \n",
    "                # Map various formats to standard CE/PE\n",
    "                option_type_mapping = {\n",
    "                    'CALL': 'CE', 'C': 'CE', 'CALL_OPTION': 'CE',\n",
    "                    'PUT': 'PE', 'P': 'PE', 'PUT_OPTION': 'PE'\n",
    "                }\n",
    "                \n",
    "                for old_type, new_type in option_type_mapping.items():\n",
    "                    df_copy.loc[df_copy['option_type'] == old_type, 'option_type'] = new_type\n",
    "                \n",
    "                # Check for invalid option types\n",
    "                valid_types = ['CE', 'PE']\n",
    "                invalid_types = ~df_copy['option_type'].isin(valid_types)\n",
    "                invalid_count = invalid_types.sum()\n",
    "                \n",
    "                if invalid_count > 0:\n",
    "                    validation_issues.append(f\"Found {invalid_count} invalid option types\")\n",
    "                    df_copy = df_copy[df_copy['option_type'].isin(valid_types)]\n",
    "            \n",
    "            # Premium validation\n",
    "            if 'premium' in df_copy.columns:\n",
    "                # Premiums should be non-negative\n",
    "                negative_premiums = (df_copy['premium'] < 0).sum()\n",
    "                if negative_premiums > 0:\n",
    "                    validation_issues.append(f\"Found {negative_premiums} negative premiums\")\n",
    "                    df_copy.loc[df_copy['premium'] < 0, 'premium'] = 0\n",
    "                \n",
    "                # Check for unreasonably high premiums\n",
    "                if 'strike' in df_copy.columns:\n",
    "                    # Premium should generally not exceed strike price for reasonable options\n",
    "                    high_premium_mask = df_copy['premium'] > df_copy['strike']\n",
    "                    high_premium_count = high_premium_mask.sum()\n",
    "                    \n",
    "                    if high_premium_count > 0:\n",
    "                        validation_issues.append(f\"Found {high_premium_count} suspiciously high premiums\")\n",
    "                        # Log but don't automatically fix - might be legitimate for deep ITM options\n",
    "            \n",
    "            # Greeks validation\n",
    "            if 'delta' in df_copy.columns:\n",
    "                # Delta should be between -1 and 1\n",
    "                invalid_delta = (df_copy['delta'] < -1) | (df_copy['delta'] > 1)\n",
    "                if invalid_delta.any():\n",
    "                    validation_issues.append(f\"Found {invalid_delta.sum()} invalid delta values\")\n",
    "                    df_copy.loc[invalid_delta, 'delta'] = np.nan\n",
    "            \n",
    "            if 'gamma' in df_copy.columns:\n",
    "                # Gamma should be non-negative\n",
    "                negative_gamma = (df_copy['gamma'] < 0).sum()\n",
    "                if negative_gamma > 0:\n",
    "                    validation_issues.append(f\"Found {negative_gamma} negative gamma values\")\n",
    "                    df_copy.loc[df_copy['gamma'] < 0, 'gamma'] = np.nan\n",
    "            \n",
    "            # Implied Volatility validation\n",
    "            iv_col = 'iv' if 'iv' in df_copy.columns else 'implied_volatility' if 'implied_volatility' in df_copy.columns else None\n",
    "            if iv_col:\n",
    "                # IV should be positive and reasonable (typically < 300%)\n",
    "                invalid_iv = (df_copy[iv_col] <= 0) | (df_copy[iv_col] > 3.0)\n",
    "                if invalid_iv.any():\n",
    "                    validation_issues.append(f\"Found {invalid_iv.sum()} invalid IV values\")\n",
    "                    df_copy.loc[invalid_iv, iv_col] = np.nan\n",
    "            \n",
    "            if validation_issues:\n",
    "                logger.warning(f\"Options validation issues: {validation_issues}\")\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        options_df = safe_column_operation(options_df, validate_options_specifics)\n",
    "        \n",
    "        # 5. Clean extreme values specific to options\n",
    "        def clean_options_extremes(df):\n",
    "            \"\"\"Clean options-specific extreme values\"\"\"\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Replace infinite values\n",
    "            df_copy = df_copy.replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            # Volume and OI outlier handling for options\n",
    "            for vol_col in ['volume', 'open_interest', 'oi']:\n",
    "                if vol_col in df_copy.columns and len(df_copy) > 10:\n",
    "                    # Use percentile-based outlier detection for options volume\n",
    "                    q99 = df_copy[vol_col].quantile(0.99)\n",
    "                    q1 = df_copy[vol_col].quantile(0.01)\n",
    "                    \n",
    "                    outlier_mask = (df_copy[vol_col] > q99 * 10) | (df_copy[vol_col] < 0)\n",
    "                    outlier_count = outlier_mask.sum()\n",
    "                    \n",
    "                    if outlier_count > 0:\n",
    "                        logger.warning(f\"Capping {outlier_count} {vol_col} outliers\")\n",
    "                        df_copy.loc[df_copy[vol_col] > q99 * 10, vol_col] = q99\n",
    "                        df_copy.loc[df_copy[vol_col] < 0, vol_col] = 0\n",
    "            \n",
    "            # Fill remaining NaN values with appropriate methods\n",
    "            df_copy = df_copy.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "            return df_copy\n",
    "        \n",
    "        options_df = safe_column_operation(options_df, clean_options_extremes)\n",
    "        \n",
    "        # ===== OPTIONS PROCESSING =====\n",
    "        # Process options with specialized processor\n",
    "        processing_result = options_processor.process_options_dataframe(\n",
    "            options_df,\n",
    "            current_price=request.current_price,\n",
    "            add_greeks=True,\n",
    "            add_technical_indicators=True\n",
    "        )\n",
    "        \n",
    "        if not processing_result.success:\n",
    "            logger.warning(f\"Options processing failed: {processing_result.error_message}\")\n",
    "            processed_df = options_df\n",
    "        else:\n",
    "            processed_df = processing_result.data\n",
    "        \n",
    "        # ===== DATA QUALITY ASSESSMENT =====\n",
    "        total_rows = len(processed_df)\n",
    "        non_null_rows = processed_df.dropna().shape[0]\n",
    "        data_quality_score = (non_null_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        # Options-specific quality metrics\n",
    "        unique_strikes = len(processed_df['strike'].unique()) if 'strike' in processed_df.columns else 0\n",
    "        \n",
    "        option_types = processed_df['option_type'].value_counts().to_dict() if 'option_type' in processed_df.columns else {}\n",
    "        \n",
    "        # Calculate average volume and OI\n",
    "        avg_volume = processed_df['volume'].mean() if 'volume' in processed_df.columns else 0\n",
    "        avg_oi = processed_df['open_interest'].mean() if 'open_interest' in processed_df.columns else (\n",
    "            processed_df['oi'].mean() if 'oi' in processed_df.columns else 0\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"🔄 Options Data Quality Assessment:\")\n",
    "        logger.info(f\"  Total rows: {total_rows:,}\")\n",
    "        logger.info(f\"  Complete rows: {non_null_rows:,}\")\n",
    "        logger.info(f\"  Quality score: {data_quality_score:.2f}%\")\n",
    "        logger.info(f\"  Unique strikes: {unique_strikes}\")\n",
    "        logger.info(f\"  Option types: {option_types}\")\n",
    "        logger.info(f\"  Average volume: {avg_volume:.2f}\")\n",
    "        logger.info(f\"  Average OI: {avg_oi:.2f}\")\n",
    "        \n",
    "        # Save with comprehensive metadata\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            processed_df,\n",
    "            \"tcs_options_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"options\",\n",
    "                \"stock_code\": request.stock_code,\n",
    "                \"expiry_date\": request.expiry_date,\n",
    "                \"current_price\": request.current_price,\n",
    "                \"validation_timestamp\": datetime.now().isoformat(),\n",
    "                \"data_quality_score\": data_quality_score,\n",
    "                \"unique_strikes\": unique_strikes,\n",
    "                \"option_types\": option_types,\n",
    "                \"average_volume\": avg_volume,\n",
    "                \"average_oi\": avg_oi,\n",
    "                \"total_records\": total_rows,\n",
    "                \"complete_records\": non_null_rows,\n",
    "                \"validation_checks_passed\": [\n",
    "                    \"column_structure\", \"datetime_format\", \"numeric_types\",\n",
    "                    \"strike_validation\", \"option_type_standardization\", \"premium_validation\",\n",
    "                    \"greeks_validation\", \"iv_validation\"\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"✅ Options data saved: {len(processed_df)} records, {unique_strikes} strikes\")\n",
    "        else:\n",
    "            logger.warning(f\"Options save failed: {save_result.error_message}\")\n",
    "        \n",
    "        logger.info(f\"✅ Options data validation completed successfully\")\n",
    "        return processed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Options data processing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Fetch options data with validation\n",
    "options_df = fetch_options_data(market_request)\n",
    "if options_df is not None:\n",
    "    logger.info(f\"🔄 Options data shape: {options_df.shape}\")\n",
    "    if 'option_type' in options_df.columns:\n",
    "        logger.info(f\"🔄 Option breakdown: {options_df['option_type'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    logger.warning(\"⚠️ Options data not available, continuing without it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe69aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 🔗 Data Combination and Enhancement\n",
    "# =====================================\n",
    "\n",
    "from data_processing_utils import ProcessingResult, DataQuality\n",
    "\n",
    "def combine_and_enhance_data(equity_df, futures_df=None, options_df=None):\n",
    "    \"\"\"Combine and enhance all datasets with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(\"🔗 Starting data combination and enhancement\")\n",
    "        \n",
    "        # Use enhanced options processor for data combination\n",
    "        combination_result = options_processor.combine_market_data(\n",
    "            equity_data=equity_df,\n",
    "            futures_data=futures_df,\n",
    "            options_data=options_df\n",
    "        )\n",
    "        \n",
    "        if not combination_result.success:\n",
    "            raise ValidationError(f\"Data combination failed: {combination_result.error_message}\")\n",
    "        \n",
    "        combined_df = combination_result.data\n",
    "        logger.info(f\"✅ Data combined successfully: {combined_df.shape}\")\n",
    "        \n",
    "        # Enhance with relationship metadata using options processor\n",
    "        enhancement_result = options_processor.add_relationship_features(\n",
    "            combined_df,\n",
    "            include_correlations=True,\n",
    "            include_price_targets=True\n",
    "        )\n",
    "        \n",
    "        if not enhancement_result.success:\n",
    "            logger.warning(f\"Enhancement failed: {enhancement_result.error_message}\")\n",
    "            enhanced_df = combined_df\n",
    "        else:\n",
    "            enhanced_df = enhancement_result.data\n",
    "        \n",
    "        # Data quality assessment\n",
    "        quality_result = options_processor.assess_data_quality(enhanced_df)\n",
    "        logger.info(f\"📊 Data quality: {quality_result.metadata.get('quality_score', 'N/A')}\")\n",
    "        \n",
    "        # Save final enhanced dataset\n",
    "        save_result = data_manager.save_dataframe(\n",
    "            enhanced_df,\n",
    "            \"tcs_enhanced_data.csv\",\n",
    "            metadata={\n",
    "                \"source\": \"combined_enhanced\",\n",
    "                \"features_count\": len(enhanced_df.columns),\n",
    "                \"records_count\": len(enhanced_df),\n",
    "                \"data_quality\": quality_result.metadata.get('quality_score'),\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if save_result.success:\n",
    "            logger.info(f\"✅ Enhanced dataset saved: {enhanced_df.shape} with {len(enhanced_df.columns)} features\")\n",
    "        else:\n",
    "            logger.warning(f\"Enhanced data save failed: {save_result.error_message}\")\n",
    "        \n",
    "        return enhanced_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data combination and enhancement failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Combine and enhance all data\n",
    "try:\n",
    "    enhanced_df = combine_and_enhance_data(equity_df, futures_df, options_df)\n",
    "    \n",
    "    # Final data summary\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"📊 FINAL DATA SUMMARY\")\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(f\"📈 Total records: {len(enhanced_df):,}\")\n",
    "    logger.info(f\"📊 Total features: {len(enhanced_df.columns):,}\")\n",
    "    logger.info(f\"📅 Date range: {enhanced_df['datetime'].min()} to {enhanced_df['datetime'].max()}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    equity_features = len([c for c in enhanced_df.columns if c.startswith('equity_')])\n",
    "    futures_features = len([c for c in enhanced_df.columns if c.startswith('futures_')])\n",
    "    options_features = len([c for c in enhanced_df.columns if c.startswith('options_')])\n",
    "    relationship_features = len([c for c in enhanced_df.columns if any(keyword in c for keyword in ['corr_', 'basis_', 'divergence', 'ratio'])])\n",
    "    \n",
    "    logger.info(f\"📈 Equity features: {equity_features}\")\n",
    "    logger.info(f\"📊 Futures features: {futures_features}\")\n",
    "    logger.info(f\"🔄 Options features: {options_features}\")\n",
    "    logger.info(f\"🔗 Relationship features: {relationship_features}\")\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"✅✅✅ ALL DATA PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Critical error in data processing: {str(e)}\")\n",
    "    # Provide graceful fallback\n",
    "    if 'equity_df' in locals():\n",
    "        logger.info(\"📈 Falling back to equity data only\")\n",
    "        enhanced_df = equity_df\n",
    "    else:\n",
    "        logger.error(\"💥 Complete failure - no data available\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
