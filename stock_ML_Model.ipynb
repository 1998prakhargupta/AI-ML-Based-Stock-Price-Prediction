{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FGAXD-bK9PA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"Warning: matplotlib not available, plotting functions will be disabled\")\n",
    "    plt = None\n",
    "\n",
    "from app_config import Config\n",
    "from model_utils import ModelDataProcessor, ModelEvaluator, ModelManager\n",
    "\n",
    "# Initialize secure configuration and utilities\n",
    "config = Config()\n",
    "data_processor = ModelDataProcessor()\n",
    "evaluator = ModelEvaluator()\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Mount Google Drive if in Colab environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Not in Colab environment, skipping Google Drive mount\")\n",
    "\n",
    "print(\"‚úÖ All modules and utilities loaded successfully!\")\n",
    "\n",
    "class MemoryEfficientStockPredictor:\n",
    "    def __init__(self, look_back=60):\n",
    "        self.look_back = look_back\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.weights = {}\n",
    "        # Updated scaler patterns to match your naming convention\n",
    "        self.scaler_patterns = ['_minmax', '_robust', '_z', '_std']\n",
    "        self.equity_cols = []\n",
    "        self.futures_cols = []\n",
    "        self.options_cols = []\n",
    "        self.nse_index_cols = []\n",
    "        self.technical_indicator_cols = []\n",
    "\n",
    "    def load_data(self, train_path, test_path):\n",
    "        \"\"\"Load data efficiently with memory management\"\"\"\n",
    "        print(\"Loading training data...\")\n",
    "        self.train_data = pd.read_parquet(train_path)\n",
    "        print(f\"Training data shape: {self.train_data.shape}\")\n",
    "\n",
    "        print(\"Loading test data...\")\n",
    "        self.test_data = pd.read_parquet(test_path)\n",
    "        print(f\"Test data shape: {self.test_data.shape}\")\n",
    "\n",
    "        # Categorize columns based on your data structure\n",
    "        self._categorize_columns(self.train_data.columns)\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "    def _categorize_columns(self, columns):\n",
    "        \"\"\"Categorize columns based on data type and naming patterns\"\"\"\n",
    "        print(\"Categorizing data columns...\")\n",
    "\n",
    "        # Reset column lists\n",
    "        self.equity_cols = []\n",
    "        self.futures_cols = []\n",
    "        self.options_cols = []\n",
    "        self.nse_index_cols = []\n",
    "        self.technical_indicator_cols = []\n",
    "\n",
    "        # Common technical indicators you mentioned\n",
    "        tech_indicators = ['RSI', 'MACD', 'SMA', 'EMA', 'BB', 'ATR', 'ADX', 'STOCH', 'CCI', 'MFI',\n",
    "                          'OBV', 'VWAP', 'BOLL', 'SAR', 'WILLIAMS', 'ROC', 'MOM', 'TSI', 'UO',\n",
    "                          'crossover', 'divergence', 'weight_opt', 'strategy', 'oi_buildup',\n",
    "                          'unusual_volume', 'rolling_mean', 'rolling_var', 'rolling_std']\n",
    "\n",
    "        option_keywords = ['CE', 'PE', 'CALL', 'PUT', 'STRIKE', 'EXPIRY', 'IV', 'DELTA', 'GAMMA',\n",
    "                          'THETA', 'VEGA', 'RHO', 'option_chain', 'pcr', 'max_pain']\n",
    "\n",
    "        futures_keywords = ['FUT', 'FUTURE', 'basis', 'contango', 'backwardation', 'roll_yield']\n",
    "\n",
    "        nse_index_keywords = ['NIFTY', 'SENSEX', 'BANKNIFTY', 'INDEX', 'VIX']\n",
    "\n",
    "        for col in columns:\n",
    "            col_upper = col.upper()\n",
    "\n",
    "            # Check for technical indicators\n",
    "            if any(indicator.upper() in col_upper for indicator in tech_indicators):\n",
    "                self.technical_indicator_cols.append(col)\n",
    "\n",
    "            # Check for options data\n",
    "            elif any(keyword in col_upper for keyword in option_keywords):\n",
    "                self.options_cols.append(col)\n",
    "\n",
    "            # Check for futures data\n",
    "            elif any(keyword in col_upper for keyword in futures_keywords):\n",
    "                self.futures_cols.append(col)\n",
    "\n",
    "            # Check for NSE indices\n",
    "            elif any(index_name in col_upper for index_name in nse_index_keywords):\n",
    "                self.nse_index_cols.append(col)\n",
    "\n",
    "            # Default to equity if basic OHLCV pattern\n",
    "            elif any(ohlcv in col_upper for ohlcv in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME']):\n",
    "                self.equity_cols.append(col)\n",
    "\n",
    "        print(f\"Equity columns: {len(self.equity_cols)}\")\n",
    "        print(f\"Futures columns: {len(self.futures_cols)}\")\n",
    "        print(f\"Options columns: {len(self.options_cols)}\")\n",
    "        print(f\"NSE Index columns: {len(self.nse_index_cols)}\")\n",
    "        print(f\"Technical Indicator columns: {len(self.technical_indicator_cols)}\")\n",
    "        print(f\"Total columns: {len(columns)}\")\n",
    "\n",
    "    def prepare_features(self, data, target_col='Close'):\n",
    "        \"\"\"Prepare features for different model types using existing calculations\"\"\"\n",
    "        print(\"Preparing features from existing calculations...\")\n",
    "\n",
    "        # Since you've already calculated most indicators, we'll use them directly\n",
    "        # Get all scaled columns based on your naming convention\n",
    "        scaled_cols = []\n",
    "        for col in data.columns:\n",
    "            if any(pattern in col for pattern in self.scaler_patterns):\n",
    "                scaled_cols.append(col)\n",
    "\n",
    "        print(f\"Found {len(scaled_cols)} scaled columns\")\n",
    "\n",
    "        # Create feature groups for different model types\n",
    "        feature_groups = {\n",
    "            'equity_features': self.equity_cols,\n",
    "            'futures_features': self.futures_cols,\n",
    "            'options_features': self.options_cols,\n",
    "            'nse_index_features': self.nse_index_cols,\n",
    "            'technical_features': self.technical_indicator_cols,\n",
    "            'scaled_features': scaled_cols\n",
    "        }\n",
    "\n",
    "        # Combine all available features (excluding target)\n",
    "        all_features = []\n",
    "        for group_name, features in feature_groups.items():\n",
    "            available_features = [col for col in features if col in data.columns and col != target_col]\n",
    "            all_features.extend(available_features)\n",
    "            print(f\"{group_name}: {len(available_features)} features\")\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        all_features = list(dict.fromkeys(all_features))\n",
    "\n",
    "        # Add any remaining numeric columns that might be useful\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        for col in numeric_cols:\n",
    "            if col not in all_features and col != target_col:\n",
    "                all_features.append(col)\n",
    "\n",
    "        print(f\"Total features selected: {len(all_features)}\")\n",
    "\n",
    "        # Ensure target column exists\n",
    "        if target_col not in data.columns:\n",
    "            print(f\"Warning: Target column '{target_col}' not found. Using first equity close column.\")\n",
    "            close_cols = [col for col in data.columns if 'close' in col.lower()]\n",
    "            if close_cols:\n",
    "                target_col = close_cols[0]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable target column found\")\n",
    "\n",
    "        # Create final dataset with features and target\n",
    "        final_features = [col for col in all_features if col in data.columns]\n",
    "        final_data = data[final_features + [target_col]].copy()\n",
    "\n",
    "        # Handle any remaining NaN values\n",
    "        final_data = final_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # Remove any remaining NaN rows\n",
    "        final_data = final_data.dropna()\n",
    "\n",
    "        print(f\"Final prepared data shape: {final_data.shape}\")\n",
    "        return final_data\n",
    "\n",
    "    def calculate_rsi(self, prices, window=14):\n",
    "        \"\"\"Calculate RSI indicator\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def calculate_macd(self, prices, fast=12, slow=26):\n",
    "        \"\"\"Calculate MACD indicator\"\"\"\n",
    "        ema_fast = prices.ewm(span=fast).mean()\n",
    "        ema_slow = prices.ewm(span=slow).mean()\n",
    "        return ema_fast - ema_slow\n",
    "\n",
    "    def create_sequences(self, data, target_col='Close'):\n",
    "        \"\"\"Create sequences for LSTM/GRU models with smart feature selection\"\"\"\n",
    "        print(\"Creating sequences for deep learning models...\")\n",
    "\n",
    "        # Select most important features for sequence models to manage memory\n",
    "        # Prioritize scaled features and key technical indicators\n",
    "        priority_features = []\n",
    "\n",
    "        # Add scaled features (these are already normalized)\n",
    "        for col in data.columns:\n",
    "            if any(pattern in col for pattern in self.scaler_patterns):\n",
    "                priority_features.append(col)\n",
    "\n",
    "        # Add key technical indicators\n",
    "        key_indicators = ['RSI', 'MACD', 'SMA', 'EMA', 'ATR', 'VWAP', 'BOLL']\n",
    "        for col in data.columns:\n",
    "            if any(indicator in col.upper() for indicator in key_indicators) and col not in priority_features:\n",
    "                priority_features.append(col)\n",
    "\n",
    "        # Add volume and OI related features\n",
    "        volume_oi_features = ['VOLUME', 'OI', 'BUILDUP', 'UNUSUAL']\n",
    "        for col in data.columns:\n",
    "            if any(feature in col.upper() for feature in volume_oi_features) and col not in priority_features:\n",
    "                priority_features.append(col)\n",
    "\n",
    "        # Limit features to prevent memory issues (max 50 features for sequences)\n",
    "        if len(priority_features) > 50:\n",
    "            priority_features = priority_features[:50]\n",
    "\n",
    "        # Ensure we have the features in the data\n",
    "        available_features = [col for col in priority_features if col in data.columns and col != target_col]\n",
    "\n",
    "        print(f\"Using {len(available_features)} features for sequence creation\")\n",
    "\n",
    "        if not available_features:\n",
    "            # Fallback to all numeric columns except target\n",
    "            available_features = [col for col in data.select_dtypes(include=[np.number]).columns\n",
    "                                if col != target_col][:30]  # Limit to 30 features\n",
    "\n",
    "        sequences = []\n",
    "        targets = []\n",
    "\n",
    "        for i in range(self.look_back, len(data)):\n",
    "            sequences.append(data[available_features].iloc[i-self.look_back:i].values)\n",
    "            targets.append(data[target_col].iloc[i])\n",
    "\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    def train_arima(self, data, target_col='Close'):\n",
    "        \"\"\"Train ARIMA model with memory efficiency\"\"\"\n",
    "        print(\"Training ARIMA model...\")\n",
    "        try:\n",
    "            # Use a subset for ARIMA if data is too large\n",
    "            if len(data) > 5000:\n",
    "                data_subset = data.tail(5000)\n",
    "            else:\n",
    "                data_subset = data\n",
    "\n",
    "            model = ARIMA(data_subset[target_col], order=(2, 1, 2))\n",
    "            fitted_model = model.fit()\n",
    "            self.models['ARIMA'] = fitted_model\n",
    "            print(\"ARIMA model trained successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"ARIMA training failed: {e}\")\n",
    "            self.models['ARIMA'] = None\n",
    "\n",
    "    def train_lstm(self, X_seq, y_seq):\n",
    "        \"\"\"Train Bi-LSTM model\"\"\"\n",
    "        print(\"Training Bi-LSTM model...\")\n",
    "\n",
    "        # Clear any existing models from memory\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(50, return_sequences=True), input_shape=(X_seq.shape[1], X_seq.shape[2])),\n",
    "            Dropout(0.2),\n",
    "            Bidirectional(LSTM(50, return_sequences=False)),\n",
    "            Dropout(0.2),\n",
    "            Dense(25),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Split data for validation\n",
    "        split_idx = int(0.8 * len(X_seq))\n",
    "        X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
    "        y_train, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=32,\n",
    "                 callbacks=[early_stop], verbose=0)\n",
    "\n",
    "        self.models['BiLSTM'] = model\n",
    "        print(\"Bi-LSTM model trained successfully\")\n",
    "\n",
    "        # Clear memory\n",
    "        del X_train, X_val, y_train, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    def train_gru(self, X_seq, y_seq):\n",
    "        \"\"\"Train GRU model\"\"\"\n",
    "        print(\"Training GRU model...\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(50, return_sequences=True, input_shape=(X_seq.shape[1], X_seq.shape[2])),\n",
    "            Dropout(0.2),\n",
    "            GRU(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(25),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        split_idx = int(0.8 * len(X_seq))\n",
    "        X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
    "        y_train, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 epochs=50, batch_size=32,\n",
    "                 callbacks=[early_stop], verbose=0)\n",
    "\n",
    "        self.models['GRU'] = model\n",
    "        print(\"GRU model trained successfully\")\n",
    "\n",
    "        del X_train, X_val, y_train, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    def train_tree_models(self, X_train, y_train):\n",
    "        \"\"\"Train tree-based models with feature importance analysis\"\"\"\n",
    "        print(\"Training tree-based models...\")\n",
    "\n",
    "        # Reduce features if too many to prevent overfitting and memory issues\n",
    "        if X_train.shape[1] > 100:\n",
    "            print(f\"Reducing features from {X_train.shape[1]} to top 100 most important\")\n",
    "            # Quick feature selection using correlation with target\n",
    "            correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "            top_features = correlations.head(100).index.tolist()\n",
    "            X_train_reduced = X_train[top_features]\n",
    "        else:\n",
    "            X_train_reduced = X_train\n",
    "\n",
    "        print(f\"Training with {X_train_reduced.shape[1]} features\")\n",
    "\n",
    "        # XGBoost with parameters optimized for your multi-asset data\n",
    "        self.models['XGBoost'] = xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1\n",
    "        )\n",
    "        self.models['XGBoost'].fit(X_train_reduced, y_train)\n",
    "\n",
    "        # LightGBM optimized for financial data\n",
    "        self.models['LightGBM'] = lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            verbose=-1,\n",
    "            importance_type='gain'\n",
    "        )\n",
    "        self.models['LightGBM'].fit(X_train_reduced, y_train)\n",
    "\n",
    "        # Random Forest with parameters for financial time series\n",
    "        self.models['RandomForest'] = RandomForestRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.models['RandomForest'].fit(X_train_reduced, y_train)\n",
    "\n",
    "        # Store the selected features for prediction\n",
    "        self.tree_model_features = X_train_reduced.columns.tolist()\n",
    "\n",
    "        print(\"Tree-based models trained successfully\")\n",
    "\n",
    "        # Print feature importance for top model\n",
    "        if hasattr(self.models['LightGBM'], 'feature_importances_'):\n",
    "            importances = self.models['LightGBM'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.tree_model_features,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            print(\"\\nTop 10 Most Important Features:\")\n",
    "            print(feature_importance.head(10))\n",
    "\n",
    "    def train_other_models(self, X_train, y_train):\n",
    "        \"\"\"Train SVR and KNN models\"\"\"\n",
    "        print(\"Training SVR and KNN models...\")\n",
    "\n",
    "        # Use a subset for SVR if data is too large (SVR is memory intensive)\n",
    "        if len(X_train) > 5000:\n",
    "            subset_idx = np.random.choice(len(X_train), 5000, replace=False)\n",
    "            X_subset = X_train.iloc[subset_idx]\n",
    "            y_subset = y_train.iloc[subset_idx]\n",
    "        else:\n",
    "            X_subset = X_train\n",
    "            y_subset = y_train\n",
    "\n",
    "        # SVR\n",
    "        self.models['SVR'] = SVR(kernel='rbf', C=100, gamma='scale')\n",
    "        self.models['SVR'].fit(X_subset, y_subset)\n",
    "\n",
    "        # KNN\n",
    "        self.models['KNN'] = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "        self.models['KNN'].fit(X_train, y_train)\n",
    "\n",
    "        print(\"SVR and KNN models trained successfully\")\n",
    "\n",
    "    def train_all_models(self):\n",
    "        \"\"\"Train all models with memory management\"\"\"\n",
    "        # Prepare data\n",
    "        print(\"Preparing training data...\")\n",
    "        train_data = self.prepare_features(self.train_data.copy())\n",
    "\n",
    "        # Prepare features for non-sequential models\n",
    "        feature_cols = [col for col in train_data.columns if col not in ['Close']]\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['Close']\n",
    "\n",
    "        # Train ARIMA\n",
    "        self.train_arima(train_data)\n",
    "\n",
    "        # Prepare sequences for LSTM/GRU\n",
    "        print(\"Preparing sequences for deep learning models...\")\n",
    "        X_seq, y_seq = self.create_sequences(train_data)\n",
    "\n",
    "        # Train deep learning models\n",
    "        self.train_lstm(X_seq, y_seq)\n",
    "        self.train_gru(X_seq, y_seq)\n",
    "\n",
    "        # Clear sequences from memory\n",
    "        del X_seq, y_seq\n",
    "        gc.collect()\n",
    "\n",
    "        # Train tree-based models\n",
    "        self.train_tree_models(X_train, y_train)\n",
    "\n",
    "        # Train other models\n",
    "        self.train_other_models(X_train, y_train)\n",
    "\n",
    "        print(\"All models trained successfully!\")\n",
    "\n",
    "    def predict_single_model(self, model_name, test_data):\n",
    "        \"\"\"Make predictions with a single model\"\"\"\n",
    "        if model_name not in self.models or self.models[model_name] is None:\n",
    "            return None\n",
    "\n",
    "        model = self.models[model_name]\n",
    "\n",
    "        if model_name == 'ARIMA':\n",
    "            try:\n",
    "                forecast = model.forecast(steps=len(test_data))\n",
    "                return forecast\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        elif model_name in ['BiLSTM', 'GRU']:\n",
    "            X_seq, _ = self.create_sequences(test_data)\n",
    "            if len(X_seq) == 0:\n",
    "                return None\n",
    "            predictions = model.predict(X_seq, verbose=0)\n",
    "            return predictions.flatten()\n",
    "\n",
    "        else:  # Tree-based and other models\n",
    "            if model_name in ['XGBoost', 'LightGBM', 'RandomForest'] and hasattr(self, 'tree_model_features'):\n",
    "                # Use the same features that were selected during training\n",
    "                available_features = [col for col in self.tree_model_features if col in test_data.columns]\n",
    "                if len(available_features) != len(self.tree_model_features):\n",
    "                    print(f\"Warning: Only {len(available_features)}/{len(self.tree_model_features)} features available for {model_name}\")\n",
    "                X_test = test_data[available_features]\n",
    "            else:\n",
    "                # For SVR and KNN, use all available features except target\n",
    "                feature_cols = [col for col in test_data.columns if col not in ['Close']]\n",
    "                X_test = test_data[feature_cols]\n",
    "\n",
    "            return model.predict(X_test)\n",
    "\n",
    "    def calculate_model_weights(self, validation_data):\n",
    "        \"\"\"Calculate weights for ensemble based on validation performance\"\"\"\n",
    "        print(\"Calculating model weights...\")\n",
    "\n",
    "        val_data = self.prepare_features(validation_data.copy())\n",
    "        val_target = val_data['Close'].values\n",
    "\n",
    "        weights = {}\n",
    "        errors = {}\n",
    "\n",
    "        for model_name in self.models.keys():\n",
    "            if self.models[model_name] is not None:\n",
    "                try:\n",
    "                    pred = self.predict_single_model(model_name, val_data)\n",
    "                    if pred is not None:\n",
    "                        # Align predictions with targets\n",
    "                        if model_name in ['BiLSTM', 'GRU']:\n",
    "                            aligned_target = val_target[self.look_back:]\n",
    "                        elif model_name == 'ARIMA':\n",
    "                            aligned_target = val_target[-len(pred):]\n",
    "                        else:\n",
    "                            aligned_target = val_target\n",
    "\n",
    "                        if len(pred) == len(aligned_target):\n",
    "                            mse = mean_squared_error(aligned_target, pred)\n",
    "                            errors[model_name] = mse\n",
    "                            weights[model_name] = 1 / (mse + 1e-6)  # Inverse of error\n",
    "                        else:\n",
    "                            weights[model_name] = 0\n",
    "                    else:\n",
    "                        weights[model_name] = 0\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating weight for {model_name}: {e}\")\n",
    "                    weights[model_name] = 0\n",
    "            else:\n",
    "                weights[model_name] = 0\n",
    "\n",
    "        # Normalize weights\n",
    "        total_weight = sum(weights.values())\n",
    "        if total_weight > 0:\n",
    "            self.weights = {k: v/total_weight for k, v in weights.items()}\n",
    "        else:\n",
    "            # Equal weights if all models failed\n",
    "            self.weights = {k: 1/len(weights) for k in weights.keys()}\n",
    "\n",
    "        print(\"Model weights calculated:\")\n",
    "        for model, weight in self.weights.items():\n",
    "            print(f\"{model}: {weight:.4f}\")\n",
    "\n",
    "    def make_ensemble_predictions(self, test_data):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        print(\"Making ensemble predictions...\")\n",
    "\n",
    "        test_data_prepared = self.prepare_features(test_data.copy())\n",
    "        all_predictions = {}\n",
    "\n",
    "        # Get predictions from each model\n",
    "        for model_name in self.models.keys():\n",
    "            pred = self.predict_single_model(model_name, test_data_prepared)\n",
    "            if pred is not None:\n",
    "                all_predictions[model_name] = pred\n",
    "                print(f\"{model_name} predictions shape: {len(pred)}\")\n",
    "\n",
    "        # Create ensemble prediction\n",
    "        if not all_predictions:\n",
    "            raise ValueError(\"No valid predictions from any model\")\n",
    "\n",
    "        # Find the minimum length to align all predictions\n",
    "        min_length = min(len(pred) for pred in all_predictions.values())\n",
    "\n",
    "        ensemble_pred = np.zeros(min_length)\n",
    "\n",
    "        for model_name, pred in all_predictions.items():\n",
    "            weight = self.weights.get(model_name, 0)\n",
    "            if weight > 0:\n",
    "                # Take the last min_length predictions\n",
    "                aligned_pred = pred[-min_length:] if len(pred) > min_length else pred\n",
    "                ensemble_pred += weight * aligned_pred\n",
    "\n",
    "        self.predictions = all_predictions\n",
    "        return ensemble_pred\n",
    "\n",
    "    def evaluate_predictions(self, y_true, y_pred, model_name=\"Ensemble\"):\n",
    "        \"\"\"Evaluate prediction performance\"\"\"\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"\\n{model_name} Performance:\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"R¬≤: {r2:.6f}\")\n",
    "\n",
    "        return {'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "    def plot_predictions(self, y_true, y_pred, title=\"Stock Price Predictions\"):\n",
    "        \"\"\"Plot actual vs predicted prices with additional analysis\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Main prediction plot\n",
    "        axes[0, 0].plot(y_true, label='Actual', alpha=0.8, linewidth=1.5)\n",
    "        axes[0, 0].plot(y_pred, label='Predicted', alpha=0.8, linewidth=1.5)\n",
    "        axes[0, 0].set_title(title)\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Price')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Residuals plot\n",
    "        residuals = y_true - y_pred\n",
    "        axes[0, 1].plot(residuals, alpha=0.7)\n",
    "        axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "        axes[0, 1].set_title('Residuals')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Residual')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Scatter plot\n",
    "        axes[1, 0].scatter(y_true, y_pred, alpha=0.6, s=1)\n",
    "        axes[1, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Actual')\n",
    "        axes[1, 0].set_ylabel('Predicted')\n",
    "        axes[1, 0].set_title('Actual vs Predicted')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Error distribution\n",
    "        axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_title('Error Distribution')\n",
    "        axes[1, 1].set_xlabel('Residual')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Additional analysis plots for model comparison\n",
    "        if hasattr(self, 'predictions') and len(self.predictions) > 1:\n",
    "            self.plot_model_comparison(y_true)\n",
    "\n",
    "    def plot_model_comparison(self, y_true):\n",
    "        \"\"\"Plot comparison of individual model performances\"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "        # Plot all model predictions\n",
    "        axes[0].plot(y_true, label='Actual', linewidth=2, alpha=0.8)\n",
    "\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        for i, (model_name, pred) in enumerate(self.predictions.items()):\n",
    "            if len(pred) == len(y_true):\n",
    "                color = colors[i % len(colors)]\n",
    "                axes[0].plot(pred, label=f'{model_name}', alpha=0.6, color=color)\n",
    "\n",
    "        axes[0].set_title('All Model Predictions Comparison')\n",
    "        axes[0].set_xlabel('Time')\n",
    "        axes[0].set_ylabel('Price')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot model weights\n",
    "        if hasattr(self, 'weights') and self.weights:\n",
    "            models = list(self.weights.keys())\n",
    "            weights = list(self.weights.values())\n",
    "\n",
    "            axes[1].bar(models, weights, alpha=0.7, color=colors[:len(models)])\n",
    "            axes[1].set_title('Model Weights in Ensemble')\n",
    "            axes[1].set_xlabel('Models')\n",
    "            axes[1].set_ylabel('Weight')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage Example\n",
    "def main():\n",
    "    # Initialize predictor with appropriate look_back for your time series\n",
    "    predictor = MemoryEfficientStockPredictor(look_back=20)\n",
    "\n",
    "    # Load data using secure configuration\n",
    "    config = Config()\n",
    "    train_path = config.get_model_save_path() + 'combined_training_data.parquet'\n",
    "    test_path = config.get_model_save_path() + 'batch_3.parquet'\n",
    "\n",
    "    predictor.load_data(train_path, test_path)\n",
    "\n",
    "    # Display data info\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Training data columns: {len(predictor.train_data.columns)}\")\n",
    "    print(f\"Test data columns: {len(predictor.test_data.columns)}\")\n",
    "\n",
    "    # Split training data for validation (80-20 split)\n",
    "    train_size = int(0.8 * len(predictor.train_data))\n",
    "    validation_data = predictor.train_data[train_size:].copy()\n",
    "    predictor.train_data = predictor.train_data[:train_size].copy()\n",
    "\n",
    "    print(f\"Training samples: {len(predictor.train_data)}\")\n",
    "    print(f\"Validation samples: {len(validation_data)}\")\n",
    "    print(f\"Test samples: {len(predictor.test_data)}\")\n",
    "\n",
    "    # Train all models\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    predictor.train_all_models()\n",
    "\n",
    "    # Calculate weights using validation data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CALCULATING ENSEMBLE WEIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    predictor.calculate_model_weights(validation_data)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MAKING PREDICTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    ensemble_predictions = predictor.make_ensemble_predictions(predictor.test_data)\n",
    "\n",
    "    # Prepare test targets for evaluation\n",
    "    test_data_prepared = predictor.prepare_features(predictor.test_data.copy())\n",
    "\n",
    "    # Find the appropriate target column\n",
    "    target_col = 'Close'\n",
    "    if target_col not in test_data_prepared.columns:\n",
    "        close_cols = [col for col in test_data_prepared.columns if 'close' in col.lower()]\n",
    "        if close_cols:\n",
    "            target_col = close_cols[0]\n",
    "            print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    # Align test targets with predictions\n",
    "    test_targets = test_data_prepared[target_col].values[-len(ensemble_predictions):]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Evaluate ensemble performance\n",
    "    ensemble_metrics = predictor.evaluate_predictions(test_targets, ensemble_predictions, \"Ensemble\")\n",
    "\n",
    "    # Evaluate individual models\n",
    "    print(\"\\nIndividual Model Performance:\")\n",
    "    individual_metrics = {}\n",
    "    for model_name, pred in predictor.predictions.items():\n",
    "        if len(pred) >= len(test_targets):\n",
    "            aligned_pred = pred[-len(test_targets):]\n",
    "            individual_metrics[model_name] = predictor.evaluate_predictions(\n",
    "                test_targets, aligned_pred, model_name\n",
    "            )\n",
    "\n",
    "    # Plot results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING PLOTS\")\n",
    "    print(\"=\"*50)\n",
    "    predictor.plot_predictions(test_targets, ensemble_predictions,\n",
    "                             \"Hybrid Ensemble: Equity-Futures-Options Prediction\")\n",
    "\n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best individual model: {min(individual_metrics.items(), key=lambda x: x[1]['RMSE'])[0]}\")\n",
    "    print(f\"Ensemble RMSE: {ensemble_metrics['RMSE']:.6f}\")\n",
    "    print(f\"Ensemble R¬≤: {ensemble_metrics['R2']:.6f}\")\n",
    "\n",
    "    return predictor, ensemble_predictions, test_targets\n",
    "\n",
    "# Additional utility functions for multi-asset analysis\n",
    "def analyze_data_distribution(data):\n",
    "    \"\"\"Analyze the distribution of different asset types in the dataset\"\"\"\n",
    "    print(\"Data Distribution Analysis:\")\n",
    "    print(f\"Total columns: {len(data.columns)}\")\n",
    "\n",
    "    # Count different types of columns\n",
    "    equity_count = sum(1 for col in data.columns if any(term in col.upper() for term in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME']) and not any(term in col.upper() for term in ['FUT', 'CE', 'PE']))\n",
    "    futures_count = sum(1 for col in data.columns if 'FUT' in col.upper())\n",
    "    options_count = sum(1 for col in data.columns if any(term in col.upper() for term in ['CE', 'PE', 'CALL', 'PUT']))\n",
    "    index_count = sum(1 for col in data.columns if any(term in col.upper() for term in ['NIFTY', 'SENSEX', 'BANK']))\n",
    "    scaled_count = sum(1 for col in data.columns if any(pattern in col for pattern in ['_minmax', '_robust', '_z', '_std']))\n",
    "\n",
    "    print(f\"Equity-related columns: {equity_count}\")\n",
    "    print(f\"Futures-related columns: {futures_count}\")\n",
    "    print(f\"Options-related columns: {options_count}\")\n",
    "    print(f\"Index-related columns: {index_count}\")\n",
    "    print(f\"Scaled columns: {scaled_count}\")\n",
    "\n",
    "    # Memory usage analysis\n",
    "    memory_usage = data.memory_usage(deep=True).sum() / (1024**2)  # MB\n",
    "    print(f\"Memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        'equity_count': equity_count,\n",
    "        'futures_count': futures_count,\n",
    "        'options_count': options_count,\n",
    "        'index_count': index_count,\n",
    "        'scaled_count': scaled_count,\n",
    "        'memory_mb': memory_usage\n",
    "    }\n",
    "\n",
    "def optimize_for_colab(data, max_features=200):\n",
    "    \"\"\"Optimize dataset for Colab memory constraints\"\"\"\n",
    "    print(f\"Optimizing dataset - Original shape: {data.shape}\")\n",
    "\n",
    "    # Remove highly correlated features\n",
    "    correlation_matrix = data.select_dtypes(include=[np.number]).corr().abs()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation > 0.95\n",
    "    high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "    if high_corr_features:\n",
    "        print(f\"Removing {len(high_corr_features)} highly correlated features\")\n",
    "        data = data.drop(columns=high_corr_features)\n",
    "\n",
    "    # If still too many features, select based on variance\n",
    "    if data.shape[1] > max_features:\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        variances = numeric_data.var().sort_values(ascending=False)\n",
    "        selected_features = variances.head(max_features).index.tolist()\n",
    "\n",
    "        # Always keep the target column if it exists\n",
    "        if 'Close' in data.columns and 'Close' not in selected_features:\n",
    "            selected_features.append('Close')\n",
    "\n",
    "        data = data[selected_features]\n",
    "        print(f\"Reduced to top {len(selected_features)} features by variance\")\n",
    "\n",
    "    print(f\"Optimized shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Enhanced main function with data analysis\n",
    "def main_with_analysis():\n",
    "    \"\"\"Enhanced main function with comprehensive analysis\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"MULTI-ASSET STOCK PREDICTION SYSTEM\")\n",
    "    print(\"Equity | Futures | Options | NSE Indices\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize predictor\n",
    "    predictor = MemoryEfficientStockPredictor(look_back=60)\n",
    "\n",
    "    # Load data using secure configuration\n",
    "    config = Config()\n",
    "    train_path = config.get_model_save_path() + 'combined_training_data.parquet'\n",
    "    test_path = config.get_model_save_path() + 'batch_3.parquet'\n",
    "\n",
    "    predictor.load_data(train_path, test_path)\n",
    "\n",
    "    # Analyze data distribution\n",
    "    print(\"\\nTRAINING DATA ANALYSIS:\")\n",
    "    train_analysis = analyze_data_distribution(predictor.train_data)\n",
    "\n",
    "    print(\"\\nTEST DATA ANALYSIS:\")\n",
    "    test_analysis = analyze_data_distribution(predictor.test_data)\n",
    "\n",
    "    # Optimize for Colab if needed\n",
    "    if train_analysis['memory_mb'] > 8000:  # If > 8GB\n",
    "        print(\"\\nOptimizing data for Colab memory constraints...\")\n",
    "        predictor.train_data = optimize_for_colab(predictor.train_data)\n",
    "        predictor.test_data = optimize_for_colab(predictor.test_data)\n",
    "\n",
    "    # Continue with the regular training process\n",
    "    train_size = int(0.8 * len(predictor.train_data))\n",
    "    validation_data = predictor.train_data[train_size:].copy()\n",
    "    predictor.train_data = predictor.train_data[:train_size].copy()\n",
    "\n",
    "    print(f\"\\nData Split:\")\n",
    "    print(f\"Training: {len(predictor.train_data)} samples\")\n",
    "    print(f\"Validation: {len(validation_data)} samples\")\n",
    "    print(f\"Test: {len(predictor.test_data)} samples\")\n",
    "\n",
    "    # Train models\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING HYBRID ENSEMBLE MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    predictor.train_all_models()\n",
    "\n",
    "    # Calculate ensemble weights\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OPTIMIZING ENSEMBLE WEIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    predictor.calculate_model_weights(validation_data)\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING PREDICTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    ensemble_predictions = predictor.make_ensemble_predictions(predictor.test_data)\n",
    "\n",
    "    # Evaluation\n",
    "    test_data_prepared = predictor.prepare_features(predictor.test_data.copy())\n",
    "\n",
    "    # Smart target column detection\n",
    "    target_col = 'Close'\n",
    "    if target_col not in test_data_prepared.columns:\n",
    "        close_cols = [col for col in test_data_prepared.columns if 'close' in col.lower()]\n",
    "        if close_cols:\n",
    "            target_col = close_cols[0]\n",
    "\n",
    "    test_targets = test_data_prepared[target_col].values[-len(ensemble_predictions):]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PERFORMANCE EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Ensemble evaluation\n",
    "    ensemble_metrics = predictor.evaluate_predictions(test_targets, ensemble_predictions, \"Hybrid Ensemble\")\n",
    "\n",
    "    # Individual model evaluation\n",
    "    individual_metrics = {}\n",
    "    print(\"\\nIndividual Model Performance:\")\n",
    "    for model_name, pred in predictor.predictions.items():\n",
    "        if len(pred) >= len(test_targets):\n",
    "            aligned_pred = pred[-len(test_targets):]\n",
    "            individual_metrics[model_name] = predictor.evaluate_predictions(\n",
    "                test_targets, aligned_pred, model_name\n",
    "            )\n",
    "\n",
    "    # Generate comprehensive plots\n",
    "    predictor.plot_predictions(test_targets, ensemble_predictions,\n",
    "                             \"Multi-Asset Hybrid Ensemble Prediction\")\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    best_individual = min(individual_metrics.items(), key=lambda x: x[1]['RMSE'])\n",
    "    improvement = ((best_individual[1]['RMSE'] - ensemble_metrics['RMSE']) / best_individual[1]['RMSE']) * 100\n",
    "\n",
    "    print(f\"Best Individual Model: {best_individual[0]} (RMSE: {best_individual[1]['RMSE']:.6f})\")\n",
    "    print(f\"Ensemble RMSE: {ensemble_metrics['RMSE']:.6f}\")\n",
    "    print(f\"Improvement over best individual: {improvement:.2f}%\")\n",
    "    print(f\"Ensemble R¬≤: {ensemble_metrics['R2']:.6f}\")\n",
    "\n",
    "    # Save comprehensive results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': test_targets,\n",
    "        'Ensemble': ensemble_predictions\n",
    "    })\n",
    "\n",
    "    for model_name, pred in predictor.predictions.items():\n",
    "        if len(pred) >= len(test_targets):\n",
    "            results_df[model_name] = pred[-len(test_targets):]\n",
    "\n",
    "    try:\n",
    "        results_path = config.get_model_save_path() + 'multi_asset_predictions.csv'\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"\\nDetailed results saved to: {results_path}\")\n",
    "    except:\n",
    "        print(\"Could not save results to drive\")\n",
    "\n",
    "    return predictor, ensemble_predictions, test_targets, results_df, individual_metrics\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    predictor, predictions, targets = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üìä ML Data Validation Framework\n",
    "# =====================================\n",
    "\n",
    "def validate_ml_dataframe(df: pd.DataFrame, required_columns: List[str] = None, \n",
    "                         target_column: str = 'Close') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate DataFrame for ML training with comprehensive checks.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        required_columns: List of required column names\n",
    "        target_column: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Validated DataFrame ready for ML processing\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Cannot train models on empty DataFrame\")\n",
    "    \n",
    "    print(f\"üîç Starting ML data validation for {len(df)} records...\")\n",
    "    \n",
    "    # Basic structure validation\n",
    "    if len(df) < 100:\n",
    "        print(f\"‚ö†Ô∏è Warning: Dataset has only {len(df)} records, which may be insufficient for training\")\n",
    "    \n",
    "    # Check for target column\n",
    "    if target_column not in df.columns:\n",
    "        # Try to find suitable target\n",
    "        close_cols = [col for col in df.columns if 'close' in col.lower()]\n",
    "        if close_cols:\n",
    "            target_column = close_cols[0]\n",
    "            print(f\"üéØ Using {target_column} as target column\")\n",
    "        else:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found and no suitable alternative\")\n",
    "    \n",
    "    # Validate target column\n",
    "    if df[target_column].isna().all():\n",
    "        raise ValueError(f\"Target column '{target_column}' contains only NaN values\")\n",
    "    \n",
    "    # Check for sufficient non-NaN target values\n",
    "    valid_target_count = df[target_column].notna().sum()\n",
    "    if valid_target_count < len(df) * 0.7:  # Less than 70% valid targets\n",
    "        print(f\"‚ö†Ô∏è Warning: Only {valid_target_count}/{len(df)} ({valid_target_count/len(df)*100:.1f}%) target values are valid\")\n",
    "    \n",
    "    # Validate required columns if specified\n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n",
    "            # Add missing columns with NaN\n",
    "            for col in missing_cols:\n",
    "                df[col] = np.nan\n",
    "    \n",
    "    # Remove columns with all NaN values\n",
    "    all_nan_cols = df.columns[df.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(f\"Removing {len(all_nan_cols)} columns with all NaN values\")\n",
    "        df = df.drop(columns=all_nan_cols)\n",
    "    \n",
    "    # Remove columns with single unique value (excluding NaN)\n",
    "    single_value_cols = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        unique_vals = df[col].dropna().nunique()\n",
    "        if unique_vals <= 1:\n",
    "            single_value_cols.append(col)\n",
    "    \n",
    "    if single_value_cols and target_column not in single_value_cols:\n",
    "        print(f\"Removing {len(single_value_cols)} columns with single values: {single_value_cols[:5]}{'...' if len(single_value_cols) > 5 else ''}\")\n",
    "        df = df.drop(columns=single_value_cols)\n",
    "    \n",
    "    # Handle infinite values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    inf_counts = {}\n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_counts[col] = inf_count\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if inf_counts:\n",
    "        print(f\"Replaced infinite values in {len(inf_counts)} columns\")\n",
    "    \n",
    "    # Check for extreme outliers in target\n",
    "    target_mean = df[target_column].mean()\n",
    "    target_std = df[target_column].std()\n",
    "    \n",
    "    if target_std > 0:\n",
    "        z_scores = np.abs((df[target_column] - target_mean) / target_std)\n",
    "        extreme_outliers = (z_scores > 10).sum()\n",
    "        \n",
    "        if extreme_outliers > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {extreme_outliers} extreme outliers in target column (>10 std dev)\")\n",
    "            # Cap outliers instead of removing them\n",
    "            outlier_mask = z_scores > 10\n",
    "            df.loc[outlier_mask, target_column] = target_mean + (10 * target_std * np.sign(df.loc[outlier_mask, target_column] - target_mean))\n",
    "    \n",
    "    print(f\"‚úÖ ML data validation completed: {len(df)} records, {len(df.columns)} features\")\n",
    "    return df\n",
    "\n",
    "def validate_ml_features(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Validate feature matrix and target vector for ML training.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        feature_names: List of feature names (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Validated X, y, and feature_names\n",
    "    \"\"\"\n",
    "    print(f\"üîç Validating ML features: X{X.shape}, y{y.shape if hasattr(y, 'shape') else len(y)}\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if hasattr(X, 'values'):\n",
    "        X = X.values\n",
    "    if hasattr(y, 'values'):\n",
    "        y = y.values\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Basic shape validation\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Feature matrix length ({len(X)}) doesn't match target length ({len(y)})\")\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"Empty feature matrix\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_features = np.isnan(X).any(axis=0)\n",
    "    nan_target = np.isnan(y)\n",
    "    \n",
    "    if nan_features.any():\n",
    "        nan_feature_count = nan_features.sum()\n",
    "        print(f\"‚ö†Ô∏è Found NaN values in {nan_feature_count} features\")\n",
    "        \n",
    "        # Remove features that are mostly NaN\n",
    "        nan_ratio = np.isnan(X).mean(axis=0)\n",
    "        high_nan_features = nan_ratio > 0.5\n",
    "        \n",
    "        if high_nan_features.any():\n",
    "            print(f\"Removing {high_nan_features.sum()} features with >50% NaN values\")\n",
    "            X = X[:, ~high_nan_features]\n",
    "            if feature_names:\n",
    "                feature_names = [name for i, name in enumerate(feature_names) if not high_nan_features[i]]\n",
    "        \n",
    "        # Impute remaining NaN values\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "    \n",
    "    if nan_target.any():\n",
    "        valid_mask = ~nan_target\n",
    "        print(f\"Removing {nan_target.sum()} samples with NaN target values\")\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "    \n",
    "    # Check for constant features\n",
    "    feature_std = np.std(X, axis=0)\n",
    "    constant_features = feature_std == 0\n",
    "    \n",
    "    if constant_features.any():\n",
    "        print(f\"Removing {constant_features.sum()} constant features\")\n",
    "        X = X[:, ~constant_features]\n",
    "        if feature_names:\n",
    "            feature_names = [name for i, name in enumerate(feature_names) if not constant_features[i]]\n",
    "    \n",
    "    # Final validation\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\"No valid features remaining after validation\")\n",
    "    \n",
    "    print(f\"‚úÖ Feature validation completed: X{X.shape}, y{y.shape}\")\n",
    "    return X, y, feature_names\n",
    "\n",
    "def safe_model_training(model_func, X, y, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely train a model with error handling and validation.\n",
    "    \n",
    "    Args:\n",
    "        model_func: Function to train the model\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        *args, **kwargs: Additional arguments for model_func\n",
    "    \n",
    "    Returns:\n",
    "        Trained model or None if training fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        X, y, _ = validate_ml_features(X, y)\n",
    "        \n",
    "        if len(X) < 50:\n",
    "            print(f\"‚ö†Ô∏è Warning: Training with only {len(X)} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = model_func(X, y, *args, **kwargs)\n",
    "        print(f\"‚úÖ Model training completed successfully\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ ML data validation utilities loaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
