{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Enhanced ML Model Training with Comprehensive Visualization & Reporting\n",
    "\n",
    "This notebook now includes advanced visualization and automated reporting capabilities while maintaining all existing model training and prediction logic.\n",
    "\n",
    "## Key Enhancements:\n",
    "- **Model Performance Dashboards**: Comprehensive comparison charts and metrics visualization\n",
    "- **Feature Importance Analysis**: Interactive feature importance plots and correlation heatmaps\n",
    "- **Prediction Analysis Suite**: Detailed prediction vs actual analysis with error distributions\n",
    "- **Automated Reporting**: Professional HTML reports with executive summaries\n",
    "- **Preserved Model Logic**: All existing ensemble learning and evaluation logic remains intact\n",
    "\n",
    "Let's proceed with the enhanced model training and analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FGAXD-bK9PA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"Warning: matplotlib not available, plotting functions will be disabled\")\n",
    "    plt = None\n",
    "\n",
    "from app_config import Config\n",
    "from model_utils import ModelDataProcessor, ModelEvaluator, ModelManager\n",
    "\n",
    "# üé® NEW: Import comprehensive visualization and reporting utilities\n",
    "from visualization_utils import ComprehensiveVisualizer\n",
    "from automated_reporting import AutomatedReportGenerator\n",
    "\n",
    "# Initialize secure configuration and utilities\n",
    "config = Config()\n",
    "data_processor = ModelDataProcessor()\n",
    "evaluator = ModelEvaluator()\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# üé® NEW: Initialize visualization and reporting tools\n",
    "visualizer = ComprehensiveVisualizer(config)\n",
    "report_generator = AutomatedReportGenerator(config)\n",
    "\n",
    "# Mount Google Drive if in Colab environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è Not in Colab environment, skipping Google Drive mount\")\n",
    "\n",
    "print(\"‚úÖ All modules and enhanced utilities loaded successfully!\")\n",
    "print(\"üé® Comprehensive visualization and reporting capabilities enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üìä ML Data Validation Framework\n",
    "# =====================================\n",
    "\n",
    "def validate_ml_dataframe(df: pd.DataFrame, required_columns: List[str] = None, \n",
    "                         target_column: str = 'Close') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate DataFrame for ML training with comprehensive checks.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        required_columns: List of required column names\n",
    "        target_column: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Validated DataFrame ready for ML processing\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Cannot train models on empty DataFrame\")\n",
    "    \n",
    "    print(f\"üîç Starting ML data validation for {len(df)} records...\")\n",
    "    \n",
    "    # Basic structure validation\n",
    "    if len(df) < 100:\n",
    "        print(f\"‚ö†Ô∏è Warning: Dataset has only {len(df)} records, which may be insufficient for training\")\n",
    "    \n",
    "    # Check for target column\n",
    "    if target_column not in df.columns:\n",
    "        # Try to find suitable target\n",
    "        close_cols = [col for col in df.columns if 'close' in col.lower()]\n",
    "        if close_cols:\n",
    "            target_column = close_cols[0]\n",
    "            print(f\"üéØ Using {target_column} as target column\")\n",
    "        else:\n",
    "            raise ValueError(f\"Target column '{target_column}' not found and no suitable alternative\")\n",
    "    \n",
    "    # Validate target column\n",
    "    if df[target_column].isna().all():\n",
    "        raise ValueError(f\"Target column '{target_column}' contains only NaN values\")\n",
    "    \n",
    "    # Check for sufficient non-NaN target values\n",
    "    valid_target_count = df[target_column].notna().sum()\n",
    "    if valid_target_count < len(df) * 0.7:  # Less than 70% valid targets\n",
    "        print(f\"‚ö†Ô∏è Warning: Only {valid_target_count}/{len(df)} ({valid_target_count/len(df)*100:.1f}%) target values are valid\")\n",
    "    \n",
    "    # Validate required columns if specified\n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n",
    "            # Add missing columns with NaN\n",
    "            for col in missing_cols:\n",
    "                df[col] = np.nan\n",
    "    \n",
    "    # Remove columns with all NaN values\n",
    "    all_nan_cols = df.columns[df.isna().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        print(f\"Removing {len(all_nan_cols)} columns with all NaN values\")\n",
    "        df = df.drop(columns=all_nan_cols)\n",
    "    \n",
    "    # Remove columns with single unique value (excluding NaN)\n",
    "    single_value_cols = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        unique_vals = df[col].dropna().nunique()\n",
    "        if unique_vals <= 1:\n",
    "            single_value_cols.append(col)\n",
    "    \n",
    "    if single_value_cols and target_column not in single_value_cols:\n",
    "        print(f\"Removing {len(single_value_cols)} columns with single values: {single_value_cols[:5]}{'...' if len(single_value_cols) > 5 else ''}\")\n",
    "        df = df.drop(columns=single_value_cols)\n",
    "    \n",
    "    # Handle infinite values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    inf_counts = {}\n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_counts[col] = inf_count\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if inf_counts:\n",
    "        print(f\"Replaced infinite values in {len(inf_counts)} columns\")\n",
    "    \n",
    "    # Check for extreme outliers in target\n",
    "    target_mean = df[target_column].mean()\n",
    "    target_std = df[target_column].std()\n",
    "    \n",
    "    if target_std > 0:\n",
    "        z_scores = np.abs((df[target_column] - target_mean) / target_std)\n",
    "        extreme_outliers = (z_scores > 10).sum()\n",
    "        \n",
    "        if extreme_outliers > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {extreme_outliers} extreme outliers in target column (>10 std dev)\")\n",
    "            # Cap outliers instead of removing them\n",
    "            outlier_mask = z_scores > 10\n",
    "            df.loc[outlier_mask, target_column] = target_mean + (10 * target_std * np.sign(df.loc[outlier_mask, target_column] - target_mean))\n",
    "    \n",
    "    print(f\"‚úÖ ML data validation completed: {len(df)} records, {len(df.columns)} features\")\n",
    "    return df\n",
    "\n",
    "def validate_ml_features(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Validate feature matrix and target vector for ML training.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        feature_names: List of feature names (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Validated X, y, and feature_names\n",
    "    \"\"\"\n",
    "    print(f\"üîç Validating ML features: X{X.shape}, y{y.shape if hasattr(y, 'shape') else len(y)}\")\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if hasattr(X, 'values'):\n",
    "        X = X.values\n",
    "    if hasattr(y, 'values'):\n",
    "        y = y.values\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Basic shape validation\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Feature matrix length ({len(X)}) doesn't match target length ({len(y)})\")\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"Empty feature matrix\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_features = np.isnan(X).any(axis=0)\n",
    "    nan_target = np.isnan(y)\n",
    "    \n",
    "    if nan_features.any():\n",
    "        nan_feature_count = nan_features.sum()\n",
    "        print(f\"‚ö†Ô∏è Found NaN values in {nan_feature_count} features\")\n",
    "        \n",
    "        # Remove features that are mostly NaN\n",
    "        nan_ratio = np.isnan(X).mean(axis=0)\n",
    "        high_nan_features = nan_ratio > 0.5\n",
    "        \n",
    "        if high_nan_features.any():\n",
    "            print(f\"Removing {high_nan_features.sum()} features with >50% NaN values\")\n",
    "            X = X[:, ~high_nan_features]\n",
    "            if feature_names:\n",
    "                feature_names = [name for i, name in enumerate(feature_names) if not high_nan_features[i]]\n",
    "        \n",
    "        # Impute remaining NaN values\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "    \n",
    "    if nan_target.any():\n",
    "        valid_mask = ~nan_target\n",
    "        print(f\"Removing {nan_target.sum()} samples with NaN target values\")\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "    \n",
    "    # Check for constant features\n",
    "    feature_std = np.std(X, axis=0)\n",
    "    constant_features = feature_std == 0\n",
    "    \n",
    "    if constant_features.any():\n",
    "        print(f\"Removing {constant_features.sum()} constant features\")\n",
    "        X = X[:, ~constant_features]\n",
    "        if feature_names:\n",
    "            feature_names = [name for i, name in enumerate(feature_names) if not constant_features[i]]\n",
    "    \n",
    "    # Final validation\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\"No valid features remaining after validation\")\n",
    "    \n",
    "    print(f\"‚úÖ Feature validation completed: X{X.shape}, y{y.shape}\")\n",
    "    return X, y, feature_names\n",
    "\n",
    "def safe_model_training(model_func, X, y, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely train a model with error handling and validation.\n",
    "    \n",
    "    Args:\n",
    "        model_func: Function to train the model\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        *args, **kwargs: Additional arguments for model_func\n",
    "    \n",
    "    Returns:\n",
    "        Trained model or None if training fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        X, y, _ = validate_ml_features(X, y)\n",
    "        \n",
    "        if len(X) < 50:\n",
    "            print(f\"‚ö†Ô∏è Warning: Training with only {len(X)} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = model_func(X, y, *args, **kwargs)\n",
    "        print(f\"‚úÖ Model training completed successfully\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ ML data validation utilities loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üé® COMPREHENSIVE MODEL VISUALIZATION & REPORTING\n",
    "# =====================================\n",
    "\n",
    "print(\"üé® Starting comprehensive model visualization and reporting...\")\n",
    "\n",
    "# Run the main training function first to get results\n",
    "try:\n",
    "    # Execute the main training pipeline\n",
    "    predictor, ensemble_predictions, test_targets, results_df, individual_metrics = main_with_analysis()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # =====================================\n",
    "    # üìä Model Performance Dashboard\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"üìä Creating model performance dashboard...\")\n",
    "    \n",
    "    # Create comprehensive model performance dashboard\n",
    "    dashboard_path = visualizer.create_model_performance_dashboard(\n",
    "        individual_metrics,\n",
    "        save_name=\"ml_model_performance_dashboard\"\n",
    "    )\n",
    "    \n",
    "    if dashboard_path:\n",
    "        print(f\"‚úÖ Model performance dashboard saved: {dashboard_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Dashboard creation skipped (plotting not available)\")\n",
    "    \n",
    "    # =====================================\n",
    "    # üéØ Prediction Analysis Suite\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"üéØ Creating prediction analysis suite...\")\n",
    "    \n",
    "    # Create comprehensive prediction analysis\n",
    "    prediction_suite_path = visualizer.create_prediction_analysis_suite(\n",
    "        test_targets, \n",
    "        ensemble_predictions,\n",
    "        {model_name: pred[-len(test_targets):] for model_name, pred in predictor.predictions.items() if len(pred) >= len(test_targets)},\n",
    "        save_name=\"ml_prediction_analysis_suite\"\n",
    "    )\n",
    "    \n",
    "    if prediction_suite_path:\n",
    "        print(f\"‚úÖ Prediction analysis suite saved: {prediction_suite_path}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # üß† Feature Importance Analysis\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"üß† Creating feature importance analysis...\")\n",
    "    \n",
    "    # Get feature names from the predictor\n",
    "    if hasattr(predictor, 'tree_model_features') and predictor.tree_model_features:\n",
    "        feature_names = predictor.tree_model_features\n",
    "    else:\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(50)]  # Default feature names\n",
    "    \n",
    "    # Extract models that have feature importance\n",
    "    models_with_importance = {}\n",
    "    for model_name, model in predictor.models.items():\n",
    "        if hasattr(model, 'feature_importances_') or hasattr(model, 'coef_'):\n",
    "            models_with_importance[model_name] = model\n",
    "    \n",
    "    if models_with_importance:\n",
    "        feature_analysis_path = visualizer.create_feature_importance_analysis(\n",
    "            models_with_importance,\n",
    "            feature_names,\n",
    "            save_name=\"ml_feature_importance_analysis\"\n",
    "        )\n",
    "        \n",
    "        if feature_analysis_path:\n",
    "            print(f\"‚úÖ Feature importance analysis saved: {feature_analysis_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No models with feature importance available\")\n",
    "    \n",
    "    # =====================================\n",
    "    # üìã Comprehensive Model Report\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"üìã Generating comprehensive model analysis report...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare training data for report (use a sample if too large)\n",
    "        if hasattr(predictor, 'train_data') and predictor.train_data is not None:\n",
    "            sample_size = min(1000, len(predictor.train_data))  # Limit for performance\n",
    "            training_sample = predictor.train_data.sample(n=sample_size) if len(predictor.train_data) > sample_size else predictor.train_data\n",
    "        else:\n",
    "            # Create minimal training data representation\n",
    "            training_sample = pd.DataFrame({'feature_1': test_targets, 'target': test_targets})\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        comprehensive_report_path = report_generator.generate_comprehensive_model_report(\n",
    "            models_dict=predictor.models,\n",
    "            results_dict=individual_metrics,\n",
    "            predictions_dict=predictor.predictions,\n",
    "            y_true=test_targets,\n",
    "            y_pred_ensemble=ensemble_predictions,\n",
    "            training_data=training_sample,\n",
    "            feature_names=feature_names,\n",
    "            report_name=\"comprehensive_ml_model_analysis\"\n",
    "        )\n",
    "        \n",
    "        if comprehensive_report_path:\n",
    "            print(f\"‚úÖ Comprehensive model report saved: {comprehensive_report_path}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Comprehensive report generation failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Comprehensive report generation failed: {str(e)}\")\n",
    "        \n",
    "        # Generate simplified prediction performance report as fallback\n",
    "        try:\n",
    "            simplified_report_path = report_generator.generate_prediction_performance_report(\n",
    "                test_targets,\n",
    "                ensemble_predictions,\n",
    "                model_name=\"Ensemble_Model\"\n",
    "            )\n",
    "            \n",
    "            if simplified_report_path:\n",
    "                print(f\"‚úÖ Simplified prediction report saved: {simplified_report_path}\")\n",
    "                \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Simplified report also failed: {str(e2)}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # üìà Model Performance Summary with Visualizations\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà ENHANCED PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Enhanced performance summary\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    # Calculate comprehensive ensemble metrics\n",
    "    ensemble_mse = mean_squared_error(test_targets, ensemble_predictions)\n",
    "    ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "    ensemble_mae = mean_absolute_error(test_targets, ensemble_predictions)\n",
    "    ensemble_r2 = r2_score(test_targets, ensemble_predictions)\n",
    "    ensemble_mape = np.mean(np.abs((test_targets - ensemble_predictions) / test_targets)) * 100\n",
    "    \n",
    "    print(f\"üéØ ENSEMBLE PERFORMANCE:\")\n",
    "    print(f\"   MSE: {ensemble_mse:.8f}\")\n",
    "    print(f\"   RMSE: {ensemble_rmse:.8f}\")\n",
    "    print(f\"   MAE: {ensemble_mae:.8f}\")\n",
    "    print(f\"   R¬≤: {ensemble_r2:.8f}\")\n",
    "    print(f\"   MAPE: {ensemble_mape:.4f}%\")\n",
    "    \n",
    "    # Best individual model\n",
    "    if individual_metrics:\n",
    "        best_model = min(individual_metrics.items(), key=lambda x: x[1].get('RMSE', float('inf')))\n",
    "        print(f\"\\nüèÜ BEST INDIVIDUAL MODEL: {best_model[0]}\")\n",
    "        print(f\"   RMSE: {best_model[1].get('RMSE', 'N/A')}\")\n",
    "        print(f\"   R¬≤: {best_model[1].get('R2', 'N/A')}\")\n",
    "        \n",
    "        # Calculate ensemble improvement\n",
    "        if ensemble_rmse < best_model[1].get('RMSE', float('inf')):\n",
    "            improvement = ((best_model[1].get('RMSE', 0) - ensemble_rmse) / best_model[1].get('RMSE', 1)) * 100\n",
    "            print(f\"   Ensemble Improvement: +{improvement:.2f}%\")\n",
    "        else:\n",
    "            print(f\"   Note: Individual model outperforms ensemble\")\n",
    "    \n",
    "    # Directional accuracy\n",
    "    if len(test_targets) > 1:\n",
    "        true_directions = np.diff(test_targets) > 0\n",
    "        pred_directions = np.diff(ensemble_predictions) > 0\n",
    "        directional_accuracy = np.mean(true_directions == pred_directions) * 100\n",
    "        print(f\"\\nüéØ DIRECTIONAL ACCURACY: {directional_accuracy:.2f}%\")\n",
    "    \n",
    "    # Model weights summary\n",
    "    if hasattr(predictor, 'weights') and predictor.weights:\n",
    "        print(f\"\\n‚öñÔ∏è ENSEMBLE WEIGHTS:\")\n",
    "        for model_name, weight in predictor.weights.items():\n",
    "            print(f\"   {model_name}: {weight:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® VISUALIZATION & REPORTING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä Generated comprehensive analysis including:\")\n",
    "    print(\"  ‚úÖ Model performance dashboard\")\n",
    "    print(\"  ‚úÖ Prediction analysis suite\")\n",
    "    print(\"  ‚úÖ Feature importance analysis\")\n",
    "    print(\"  ‚úÖ Comprehensive HTML reports\")\n",
    "    print(\"  ‚úÖ Executive summary\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # =====================================\n",
    "    # üíæ Enhanced Results Saving\n",
    "    # =====================================\n",
    "    \n",
    "    # Save enhanced results with comprehensive metadata\n",
    "    try:\n",
    "        enhanced_results = {\n",
    "            'ensemble_predictions': ensemble_predictions.tolist(),\n",
    "            'actual_values': test_targets.tolist(),\n",
    "            'individual_predictions': {\n",
    "                model_name: pred[-len(test_targets):].tolist() if len(pred) >= len(test_targets) else pred.tolist()\n",
    "                for model_name, pred in predictor.predictions.items()\n",
    "            },\n",
    "            'performance_metrics': {\n",
    "                'ensemble': {\n",
    "                    'MSE': float(ensemble_mse),\n",
    "                    'RMSE': float(ensemble_rmse),\n",
    "                    'MAE': float(ensemble_mae),\n",
    "                    'R2': float(ensemble_r2),\n",
    "                    'MAPE': float(ensemble_mape)\n",
    "                },\n",
    "                'individual_models': individual_metrics\n",
    "            },\n",
    "            'model_weights': dict(predictor.weights) if hasattr(predictor, 'weights') else {},\n",
    "            'generation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'data_points': len(test_targets),\n",
    "            'models_trained': len(predictor.models)\n",
    "        }\n",
    "        \n",
    "        # Save enhanced results with safe file management\n",
    "        enhanced_results_save = model_manager.save_model_results(\n",
    "            enhanced_results,\n",
    "            \"comprehensive_ml_results.json\",\n",
    "            metadata={\n",
    "                \"result_type\": \"comprehensive_ml_analysis\",\n",
    "                \"models_count\": len(predictor.models),\n",
    "                \"ensemble_r2\": float(ensemble_r2),\n",
    "                \"best_individual_model\": best_model[0] if 'best_model' in locals() else 'unknown'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if enhanced_results_save.success:\n",
    "            print(f\"‚úÖ Enhanced results saved: {enhanced_results_save.final_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Enhanced results save failed: {enhanced_results_save.error_message}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Enhanced results saving failed: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nüéâ COMPLETE ML PIPELINE WITH COMPREHENSIVE VISUALIZATION FINISHED!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in visualization and reporting pipeline: {str(e)}\")\n",
    "    print(\"üìä The basic model training may still have completed successfully\")\n",
    "    print(\"üîç Check individual model outputs above for results\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
